#  A Dual-Layer Framework for Detecting and Detoxifying Toxic language




This repository contains the official code for my MSc thesis on **toxic language detection and detoxification**, using a two-stage approach:

1. **Stage 1 â€“ Toxicity Classification:**  
   A ModernBERT-based model classifies toxic content using binary and fine-grained multi-label classification.

2. **Stage 2 â€“ Detoxification via Prompting and Fine-tuning:**  
   Detoxification is performed using prompting or fine-tuning with large language models (LLama-3-8B, Mistral-7B and mT0), with and without the Stage 1 output in the detox prompt.

---

## ğŸ“ Repository Structure

```bash
my-thesis-code/
â”œâ”€â”€ notebooks/               # Final project notebooks (pipeline, training, evaluation)
â”‚   â”œâ”€â”€ 01_load_dataset.ipynb
â”‚   â”œâ”€â”€ 02_classification_pipeline.ipynb
â”‚   â”œâ”€â”€ 03_stage1_classifier_inference.ipynb
â”‚   â”œâ”€â”€ 04_llama_prompting_without_stage1.ipynb
â”‚   â”œâ”€â”€ 05_llama_prompting_with_stage1.ipynb
â”‚   â”œâ”€â”€ 06_Mistral_7B_Instruct_v0_2_detox_prompt_without_stage_one.ipynb
â”‚   â”œâ”€â”€ 07_Mistral_7B_Instruct_v0_2_detox_prompt_with_stage_1.ipynb
â”‚   â”œâ”€â”€ 08_train_and_eval_mt0base_without_stage1.ipynb
â”‚   â”œâ”€â”€ 09_train_and_eval_mt0base_with_stage1.ipynb
â”‚   â””â”€â”€ 05_stage1_evaluation_hamming.ipynb
â”œâ”€â”€ experiments/             # Archived experimental training runs
â”‚   â”œâ”€â”€ train_moderbert_classifier_160k_example.ipynb
â”‚   â”œâ”€â”€ training_mt0_base_on_multilingual_paradetox.ipynb
â”‚   â””â”€â”€ training_mt0_base_on_multilingual_paradetox_with_stage1.ipynb
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .gitignore               # Ignore patterns for Git
â””â”€â”€ README.md                # Project documentation
```

---

## ğŸ§  Project Overview

This project proposes a modular, explainable pipeline for detoxifying toxic language:

- A **ModernBERT-based classifier** first identifies toxic content and categorizes it across binary and multiple labels.
- A second stage performs **detoxification**, tested with:
  - Prompt-only methods (Mistral-7B, LLaMA-3-8B)
  - Fine-tuned models (mT0-base) with and without Stage 1 context

The system was evaluated using **PAN 2024/2025 detoxification metrics** and Hamming loss for multi-label classification.

---

## ğŸ§ª Datasets Used

This project uses the following datasets:

- **[Jigsaw Toxic Comment Classification (Kaggle)](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge)**  

- **[ParadeTox (Hugging Face)](https://huggingface.co/datasets/s-nlp/paradetox)**  
 

- **[Multilingual ParadeTox (textdetox)](https://huggingface.co/datasets/textdetox/multilingual_paradetox)**  



> ğŸ“‚ Datasets are **not included** in this repo.  



---

## â–¶ï¸ How to Run Each Stage

### ğŸ§© Stage 1: Toxicity Classification

- Train ModernBERT classifier:
  - See `experiments/train_moderbert_classifier_160k_example.ipynb`
- Use classifier pipeline:
  - See `notebooks/02_classification_pipeline.ipynb`
- Inference-only version:
  - See `03_stage1_classifier_inference.ipynb`
- Evaluation with Hamming loss:
  - See `05_stage1_evaluation_hamming.ipynb`

### ğŸ§¹ Stage 2: Detoxification

#### Prompt-based:
- LLaMA detox without classification: `04_llama_prompting_without_stage1.ipynb`
- LLaMA detox with classification: `05_llama_prompting_with_stage1.ipynb`
- Mistral-7B detox without classification: `06_..._without_stage_one.ipynb`
- Mistral-7B detox with classification: `07_..._with_stage_1.ipynb`

#### Fine-tuned:
- `08_train_and_eval_mt0base_without_stage1.ipynb`
- `09_train_and_eval_mt0base_with_stage1.ipynb`

---

## ğŸ“Š Evaluation

This project uses official **PAN detoxification metrics**:
- Sentence Transformer Alignment (STA)
- Semantic Similarity
- CHRF1
- J-Score

Classification metrics:

- Precision, Recall, F1-score and Macro F1-score 
- **Hamming loss**

---

## ğŸ™‹ Contact

For questions or contributions, feel free to open an issue or contact me at:
- ğŸ“§ mahrokhhassani99@gmail.com
