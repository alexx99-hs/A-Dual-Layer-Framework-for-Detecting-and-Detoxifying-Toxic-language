{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "✅ Loading  fine-tuned mt0-base model\n",
        "\n",
        "✅ Loading  Stage 1 toxicity classifier\n",
        "\n",
        "✅ Using Stage 1 explanation in the prompt\n",
        "\n",
        "✅ Generating detoxified outputs from mt0-base\n",
        "\n",
        "✅ Saving outputs to a CSV file for scoring"
      ],
      "metadata": {
        "id": "cLJuZZn-wzUw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItCjieZmuONj"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "\n",
        "class ToxicityClassifierPipeline:\n",
        "    def __init__(self, binary_model_path, fine_model_path, device=\"cuda\"):\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.tokenizer_binary = AutoTokenizer.from_pretrained(binary_model_path)\n",
        "        self.model_binary = AutoModelForSequenceClassification.from_pretrained(binary_model_path).to(self.device)\n",
        "\n",
        "        self.tokenizer_fine = AutoTokenizer.from_pretrained(fine_model_path)\n",
        "        self.model_fine = AutoModelForSequenceClassification.from_pretrained(fine_model_path).to(self.device)\n",
        "\n",
        "        self.label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "        self.label_to_explanation = {\n",
        "            \"toxic\": \"This sentence contains general toxic language.\",\n",
        "            \"severe_toxic\": \"This sentence includes severe verbal abuse.\",\n",
        "            \"obscene\": \"This sentence contains obscene or vulgar language.\",\n",
        "            \"threat\": \"This sentence includes a threat or incites violence.\",\n",
        "            \"insult\": \"This sentence includes personal insults.\",\n",
        "            \"identity_hate\": \"This sentence attacks a person's identity.\"\n",
        "        }\n",
        "\n",
        "    def __call__(self, comment, threshold=0.5):\n",
        "        inputs = self.tokenizer_binary(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_binary(**inputs)\n",
        "            probs = torch.softmax(outputs.logits, dim=1)\n",
        "            toxic_prob = probs[:, 1].item()\n",
        "\n",
        "        binary_pred = \"toxic\" if toxic_prob >= threshold else \"non-toxic\"\n",
        "\n",
        "        if binary_pred == \"non-toxic\":\n",
        "            return {\"binary\": binary_pred, \"explanation\": None}\n",
        "\n",
        "        inputs = self.tokenizer_fine(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_fine(**inputs)\n",
        "            fine_probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]\n",
        "\n",
        "        explanation_parts = [\n",
        "            self.label_to_explanation[label]\n",
        "            for label, prob in zip(self.label_cols, fine_probs)\n",
        "            if prob >= threshold\n",
        "        ]\n",
        "\n",
        "        explanation = \" \".join(explanation_parts) if explanation_parts else \"This sentence contains toxic language.\"\n",
        "        return {\n",
        "            \"binary\": binary_pred,\n",
        "            \"explanation\": explanation\n",
        "        }\n",
        "\n",
        "# Instantiate the classifier\n",
        "pipeline = ToxicityClassifierPipeline(\n",
        "    binary_model_path=\"/content/drive/MyDrive/...\",\n",
        "    fine_model_path=\"/content/drive/MyDrive/...\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "U0X2YiXxw9Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/...\"\n",
        "mt0_model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(\"cuda\")\n",
        "mt0_tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
      ],
      "metadata": {
        "id": "El1U2rNCuq2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_mt0_prompt(toxic_sentence, explanation):\n",
        "    return (\n",
        "        f\"Detoxify the following sentence while preserving its meaning. \"\n",
        "        f\"The explanation below describes why the sentence is considered toxic:\\n\"\n",
        "        f\"Toxic: {toxic_sentence}\\n\"\n",
        "        f\"Explanation: {explanation}\\n\"\n",
        "        f\"Detoxified:\"\n",
        "    )\n",
        "\n",
        "def generate_mt0_detox(prompt, model, tokenizer):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=60,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2\n",
        "        )\n",
        "\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # ✅ Remove prompt from output if echoed\n",
        "    if \"Detoxified:\" in decoded:\n",
        "        detoxified = decoded.split(\"Detoxified:\")[-1].strip()\n",
        "    else:\n",
        "        detoxified = decoded.strip()\n",
        "\n",
        "    return detoxified\n"
      ],
      "metadata": {
        "id": "kk2xp9pHxS3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test file\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")\n",
        "\n",
        "# Generate new detox outputs\n",
        "outputs_mt0_stage1 = []\n",
        "\n",
        "for text in df[\"toxic_sentence\"]:\n",
        "    stage1 = pipeline(text)\n",
        "    explanation = stage1[\"explanation\"] if stage1[\"explanation\"] else \"This sentence contains toxic language.\"\n",
        "    prompt = build_mt0_prompt(text, explanation)\n",
        "    detox = generate_mt0_detox(prompt, mt0_model, mt0_tokenizer)\n",
        "    outputs_mt0_stage1.append(detox)\n",
        "\n",
        "# Save new outputs\n",
        "df[\"mt0_base_output_stage1\"] = outputs_mt0_stage1\n",
        "df.to_csv(\"/content/drive/MyDrive/...\", index=False)\n",
        "\n",
        "print(\"Saved to: mt0_base_output_with_stage1.csv\")\n"
      ],
      "metadata": {
        "id": "hIWoZHpQxboc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ BERTScore\n",
        "\n",
        "✅ CHRF\n",
        "\n",
        "✅ STA (using your Stage 1 classifier)\n",
        "\n",
        "✅ J-score"
      ],
      "metadata": {
        "id": "5WskAeC6zggB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bert_score sentence-transformers sacrebleu\n"
      ],
      "metadata": {
        "id": "3epBeJJJzd99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load MT0 outputs with Stage 1 explanation\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")\n",
        "\n",
        "# Confirm columns\n",
        "print(df.columns)\n",
        "# Should include: toxic_sentence, neutral_reference, mt0_base_output_stage1\n",
        "# Ensure outputs are strings and fill NaNs if needed\n",
        "df[\"mt0_base_output_stage1\"] = df[\"mt0_base_output_stage1\"].fillna(\"\").astype(str)\n",
        "df[\"neutral_reference\"] = df[\"neutral_reference\"].fillna(\"\").astype(str)\n",
        "\n"
      ],
      "metadata": {
        "id": "MdNfkoIszlPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import score\n",
        "\n",
        "P, R, F1 = score(\n",
        "    df[\"mt0_base_output_stage1\"].tolist(),\n",
        "    df[\"neutral_reference\"].tolist(),\n",
        "    lang=\"en\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "df[\"bertscore_f1\"] = F1.tolist()\n",
        "print(f\" Avg BERTScore F1: {F1.mean():.4f}\")\n"
      ],
      "metadata": {
        "id": "dM3Fhnwczouc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu.metrics import CHRF\n",
        "\n",
        "chrf = CHRF()\n",
        "sta = []\n",
        "chrf_scores = []\n",
        "\n",
        "for tox, detox, ref in zip(df[\"toxic_sentence\"], df[\"mt0_base_output_stage1\"], df[\"neutral_reference\"]):\n",
        "    result = pipeline(detox)\n",
        "    sta.append(0 if result[\"binary\"] == \"toxic\" else 1)\n",
        "    score_chrf = chrf.sentence_score(detox, [ref]).score / 100\n",
        "    chrf_scores.append(score_chrf)\n",
        "\n",
        "df[\"STA\"] = sta\n",
        "df[\"CHRF\"] = chrf_scores\n",
        "df[\"J_score\"] = (df[\"STA\"] + df[\"bertscore_f1\"] + df[\"CHRF\"]) / 3\n",
        "\n",
        "print(f\"✅ Avg J-score: {df['J_score'].mean():.4f}\")\n",
        "df.to_csv(\"/content/drive/MyDrive/...\", index=False)\n",
        "print(\" Saved to: mt0_base_with_stage1_evaluation.csv\")\n"
      ],
      "metadata": {
        "id": "3hWbENTUzwEn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}