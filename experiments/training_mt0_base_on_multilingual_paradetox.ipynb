{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwmpBKLvq_hp"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Load only the English split (400 examples)\n",
        "raw_dataset =load_dataset(\"textdetox/multilingual_paradetox\")\n",
        "en_data = raw_dataset[\"en\"]\n",
        "\n",
        "# Prepare it for seq2seq training: toxic → detoxified\n",
        "def format_example(example):\n",
        "    return {\n",
        "        \"input_text\": f\"detoxify: {example['toxic_sentence']}\",\n",
        "        \"labels\": example[\"neutral_sentence\"]\n",
        "    }\n",
        "\n",
        "train_data = Dataset.from_list(en_data)\n",
        "train_data = train_data.map(format_example, remove_columns=train_data.column_names)\n"
      ],
      "metadata": {
        "id": "hpjE5nZbxFtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Cx6Glhwq6e3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/mt0-base\")\n",
        "\n",
        "def tokenize(example):\n",
        "    inputs = tokenizer(example[\"input_text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(example[\"labels\"], padding=\"max_length\", truncation=True, max_length=256)\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "tokenized_data = train_data.map(tokenize, batched=True)\n"
      ],
      "metadata": {
        "id": "qcA38nW8xS4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/mt0-base\")\n",
        "model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "PcL1oE-1xU5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mt0_paradetox_finetuned\",\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=4,\n",
        "    learning_rate=1e-5,\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "from transformers import Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "IhvtfHPTxY4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/...\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/...\")\n"
      ],
      "metadata": {
        "id": "a5j7-ICbxf4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/...\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load EN toxic → neutral set\n",
        "ds = load_dataset(\"textdetox/multilingual_paradetox\")\n",
        "toxic_sentences = ds[\"en\"][\"toxic_sentence\"]\n",
        "references = ds[\"en\"][\"neutral_sentence\"]\n",
        "def generate_detox(texts, batch_size=8):\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(\"cuda\")\n",
        "        with torch.no_grad():\n",
        "            gen = model.generate(**inputs, max_new_tokens=128)\n",
        "        decoded = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "        outputs.extend(decoded)\n",
        "    return outputs\n",
        "\n",
        "generated_outputs = generate_detox(toxic_sentences)\n"
      ],
      "metadata": {
        "id": "0gYR8q9CDqGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'generated_outputs' and 'toxic_sentences' are defined from the previous code\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({'toxic_sentence': toxic_sentences, 'generated_output': generated_outputs})\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('generated_outputs.csv', index=False)\n",
        "\n",
        "# Download the CSV file\n",
        "from google.colab import files\n",
        "files.download('generated_outputs.csv')\n"
      ],
      "metadata": {
        "id": "yzGU1LmWNH9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save generated_outputs.csv to my drive\n",
        "\n",
        "# Save to CSV in Google Drive\n",
        "df.to_csv('/content/drive/MyDrive/...', index=False)\n"
      ],
      "metadata": {
        "id": "pC1BGL3xOu7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score\n",
        "from bert_score import score\n",
        "\n",
        "P, R, F1 = score(generated_outputs, references, lang=\"en\", verbose=True)\n",
        "print(f\"Average BERTScore F1: {F1.mean():.4f}\")\n"
      ],
      "metadata": {
        "id": "e359fAGEEa_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "generating for gpt-40 score"
      ],
      "metadata": {
        "id": "A7zBrrq4HShD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load ParadeTox EN data\n",
        "ds = load_dataset(\"textdetox/multilingual_paradetox\")\n",
        "toxic_sentences = ds[\"en\"][\"toxic_sentence\"]\n",
        "references = ds[\"en\"][\"neutral_sentence\"]\n",
        "\n",
        "# Run your model\n",
        "generated_outputs = generate_detox(toxic_sentences)  # from earlier\n",
        "\n",
        "# Create and save dataframe\n",
        "df = pd.DataFrame({\n",
        "    \"toxic_sentence\": toxic_sentences,\n",
        "    \"generated_output\": generated_outputs,\n",
        "    \"neutral_reference\": references\n",
        "})\n",
        "\n",
        "# Save to CSV (sample 50 rows for now)\n",
        "df.head(100).to_csv(\"/content/...\", index=False)\n"
      ],
      "metadata": {
        "id": "6blFQw5cHQij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/...\")\n"
      ],
      "metadata": {
        "id": "H4c_hC-NH7UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the model on test set of paradetox 2024 which is the same as 2025"
      ],
      "metadata": {
        "id": "Ht5SHgqVKU6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "ds = load_dataset(\"textdetox/multilingual_paradetox_test\")\n",
        "test_en = ds[\"en\"]"
      ],
      "metadata": {
        "id": "9rolBsLvJfdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/...\"\n",
        "\n",
        "# Load from local path properly\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path, local_files_only=True).to(\"cuda\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FUOE_oF0J0Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def generate_detox(texts, batch_size=8):\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(\"cuda\")\n",
        "        with torch.no_grad():\n",
        "            gen = model.generate(**inputs, max_new_tokens=128)\n",
        "        decoded = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "        outputs.extend(decoded)\n",
        "    return outputs\n",
        "\n",
        "generated_outputs = generate_detox(toxic_sentences)\n"
      ],
      "metadata": {
        "id": "Arc7l_KzKcdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'generated_outputs' and 'toxic_sentences' are defined from the previous code\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({'toxic_sentence': toxic_sentences, 'generated_output': generated_outputs,\"neutral_reference\": references})\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('generated_outputs_testset.csv', index=False)\n",
        "\n",
        "# Download the CSV file\n",
        "from google.colab import files\n",
        "files.download('generated_outputs_testset.csv')"
      ],
      "metadata": {
        "id": "OWht94LBSy8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import score\n",
        "\n",
        "P, R, F1 = score(generated_outputs, references, lang=\"en\", verbose=True)\n",
        "print(f\" BERTScore F1 on TEST set: {F1.mean():.4f}\")\n"
      ],
      "metadata": {
        "id": "FCa7mwlyKhZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now j score on test set:"
      ],
      "metadata": {
        "id": "BteL9TpmT8US"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers sacrebleu\n"
      ],
      "metadata": {
        "id": "GDdyjEnpLBDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the final dataset\n",
        "df = pd.read_csv(\"/content/...\")\n",
        "\n",
        "# Rename columns to match expected names\n",
        "df.rename(columns={\n",
        "    \"toxic_sentence\": \"input\",\n",
        "    \"generated_output\": \"prediction\",\n",
        "    \"neutral_reference\": \"reference\"\n",
        "}, inplace=True)\n"
      ],
      "metadata": {
        "id": "oZPKMoOEUD59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: moumt drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "C6VBLPvvVY4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers sacrebleu\n"
      ],
      "metadata": {
        "id": "fYsd1qZXcNmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/...\")  # adjust path if needed\n",
        "\n",
        "# Rename columns to standard names for code consistency\n",
        "df.rename(columns={\n",
        "    \"toxic_sentence\": \"input\",\n",
        "    \"generated_output\": \"prediction\",\n",
        "    \"neutral_reference\": \"reference\"\n",
        "}, inplace=True)\n"
      ],
      "metadata": {
        "id": "0I6ZMHdccQkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "class ToxicityClassifierPipeline:\n",
        "    def __init__(self, binary_model_path, fine_model_path, device=\"cuda\"):\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.tokenizer_binary = AutoTokenizer.from_pretrained(binary_model_path)\n",
        "        self.model_binary = AutoModelForSequenceClassification.from_pretrained(binary_model_path).to(self.device)\n",
        "\n",
        "        self.tokenizer_fine = AutoTokenizer.from_pretrained(fine_model_path)\n",
        "        self.model_fine = AutoModelForSequenceClassification.from_pretrained(fine_model_path).to(self.device)\n",
        "\n",
        "        self.label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "        self.label_to_explanation = {\n",
        "            \"toxic\": \"This sentence contains general toxic language.\",\n",
        "            \"severe_toxic\": \"This sentence contains extreme hostility or verbal abuse.\",\n",
        "            \"obscene\": \"This sentence contains obscene or vulgar language.\",\n",
        "            \"threat\": \"This sentence contains a threat or implied violence.\",\n",
        "            \"insult\": \"This sentence includes personal insults or demeaning language.\",\n",
        "            \"identity_hate\": \"This sentence attacks someone based on identity (e.g. race, gender, religion).\"\n",
        "        }\n",
        "\n",
        "    def __call__(self, comment, threshold=0.5):\n",
        "        inputs = self.tokenizer_binary(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_binary(**inputs)\n",
        "            probs = torch.softmax(outputs.logits, dim=1)\n",
        "            toxic_prob = probs[:, 1].item()\n",
        "\n",
        "        binary_pred = \"toxic\" if toxic_prob >= threshold else \"non-toxic\"\n",
        "\n",
        "        if binary_pred == \"non-toxic\":\n",
        "            return {\"binary\": binary_pred, \"subtypes\": None, \"toxic_prob\": toxic_prob, \"explanation\": None}\n",
        "\n",
        "        inputs = self.tokenizer_fine(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_fine(**inputs)\n",
        "            fine_probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]\n",
        "\n",
        "        subtypes = {\n",
        "            label: round(float(prob), 2)\n",
        "            for label, prob in zip(self.label_cols, fine_probs)\n",
        "            if prob >= threshold\n",
        "        }\n",
        "\n",
        "        explanation_parts = [self.label_to_explanation[label] for label in subtypes]\n",
        "        explanation = \" \".join(explanation_parts) if explanation_parts else None\n",
        "\n",
        "        return {\n",
        "            \"binary\": binary_pred,\n",
        "            \"subtypes\": subtypes,\n",
        "            \"toxic_prob\": toxic_prob,\n",
        "            \"explanation\": explanation\n",
        "        }\n"
      ],
      "metadata": {
        "id": "4kqiGKvsdMgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = ToxicityClassifierPipeline(\n",
        "    binary_model_path=\"/content/drive/MyDrive/..\",\n",
        "    fine_model_path=\"/content/drive/MyDrive/...\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "rv0er4wDdOvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_toxic(text):\n",
        "    result = pipeline(text)\n",
        "    return result[\"binary\"] == \"toxic\"\n",
        "\n",
        "df[\"STA\"] = [0 if is_toxic(pred) else 1 for pred in df[\"prediction\"]]\n"
      ],
      "metadata": {
        "id": "VIzX4_PsdiMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load multilingual sentence similarity model\n",
        "labse = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
        "\n",
        "emb_input = labse.encode(df[\"input\"].tolist(), convert_to_tensor=True)\n",
        "emb_pred = labse.encode(df[\"prediction\"].tolist(), convert_to_tensor=True)\n",
        "\n",
        "similarities = util.cos_sim(emb_input, emb_pred).diagonal().tolist()\n",
        "df[\"SIM\"] = similarities\n"
      ],
      "metadata": {
        "id": "U3Qpl8LdeCy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu.metrics import CHRF\n",
        "\n",
        "chrf = CHRF()\n",
        "chrf_scores = [\n",
        "    chrf.sentence_score(pred, [ref]).score / 100\n",
        "    for pred, ref in zip(df[\"prediction\"], df[\"reference\"])\n",
        "]\n",
        "df[\"CHRF\"] = chrf_scores\n"
      ],
      "metadata": {
        "id": "htbnCqlReJBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Final J-score is the average of STA, SIM, and CHRF\n",
        "df[\"J-score\"] = (df[\"STA\"] + df[\"SIM\"] + df[\"CHRF\"]) / 3\n",
        "\n",
        "# Print final average\n",
        "print(f\"✅ J-score (mean over all examples): {df['J-score'].mean():.4f}\")\n"
      ],
      "metadata": {
        "id": "kPK6mVaUeMue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"/content/...\", index=False)\n"
      ],
      "metadata": {
        "id": "w6zZzJQ1eQc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: download detox_jscore_results.csv to my local\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/...')\n"
      ],
      "metadata": {
        "id": "lcojy7KQeR2h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}