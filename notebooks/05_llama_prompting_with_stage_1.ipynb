{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.set_float32_matmul_precision(\"medium\")\n"
      ],
      "metadata": {
        "id": "EBHfyp6ipCY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qxrq58BXlIIM"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"...\")  # Replace with your real token"
      ],
      "metadata": {
        "id": "3n_ZH8sT-pg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# Load model in 8-bit to fit into Colab GPU\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_8bit=True\n",
        ")\n",
        "\n",
        "# Set to eval mode\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "EhLQRwwm-V7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kNqnRYeKlWYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "class ToxicityClassifierPipeline:\n",
        "    def __init__(self, binary_model_path, fine_model_path, device=\"cuda\"):\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.tokenizer_binary = AutoTokenizer.from_pretrained(binary_model_path)\n",
        "        self.model_binary = AutoModelForSequenceClassification.from_pretrained(binary_model_path).to(self.device)\n",
        "\n",
        "        self.tokenizer_fine = AutoTokenizer.from_pretrained(fine_model_path)\n",
        "        self.model_fine = AutoModelForSequenceClassification.from_pretrained(fine_model_path).to(self.device)\n",
        "\n",
        "        self.label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "        self.label_to_explanation = {\n",
        "            \"toxic\": \"This sentence contains general toxic language.\",\n",
        "            \"severe_toxic\": \"This sentence contains extreme hostility or verbal abuse.\",\n",
        "            \"obscene\": \"This sentence contains obscene or vulgar language.\",\n",
        "            \"threat\": \"This sentence contains a threat or implied violence.\",\n",
        "            \"insult\": \"This sentence includes personal insults or demeaning language.\",\n",
        "            \"identity_hate\": \"This sentence attacks someone based on identity (e.g. race, gender, religion).\"\n",
        "        }\n",
        "\n",
        "    def __call__(self, comment, threshold=0.5):\n",
        "        inputs = self.tokenizer_binary(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_binary(**inputs)\n",
        "            probs = torch.softmax(outputs.logits, dim=1)\n",
        "            toxic_prob = probs[:, 1].item()\n",
        "\n",
        "        binary_pred = \"toxic\" if toxic_prob >= threshold else \"non-toxic\"\n",
        "\n",
        "        if binary_pred == \"non-toxic\":\n",
        "            return {\"binary\": binary_pred, \"subtypes\": None, \"toxic_prob\": toxic_prob, \"explanation\": None}\n",
        "\n",
        "        inputs = self.tokenizer_fine(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_fine(**inputs)\n",
        "            fine_probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]\n",
        "\n",
        "        subtypes = {\n",
        "            label: round(float(prob), 2)\n",
        "            for label, prob in zip(self.label_cols, fine_probs)\n",
        "            if prob >= threshold\n",
        "        }\n",
        "\n",
        "        explanation_parts = [self.label_to_explanation[label] for label in subtypes]\n",
        "        explanation = \" \".join(explanation_parts) if explanation_parts else None\n",
        "\n",
        "        return {\n",
        "            \"binary\": binary_pred,\n",
        "            \"subtypes\": subtypes,\n",
        "            \"toxic_prob\": toxic_prob,\n",
        "            \"explanation\": explanation\n",
        "        }"
      ],
      "metadata": {
        "id": "uVTcSkRS6Q2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = ToxicityClassifierPipeline(\n",
        "    binary_model_path=\"/content/drive/MyDrive/...\",\n",
        "    fine_model_path=\"/content/drive/MyDrive/...\"\n",
        ")"
      ],
      "metadata": {
        "id": "D306gfMa6T1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "with Stage 1 :"
      ],
      "metadata": {
        "id": "pm36HegIbBxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Install Dependencies\n",
        "2.Load the LLaMA Model\n",
        "3.Load Stage 1 Classifier\n",
        "4.Define the Prompt"
      ],
      "metadata": {
        "id": "_7zhSIddcN-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_llama_prompt_with_explanation(toxic, explanation):\n",
        "    explanation = explanation.strip() if explanation else \"This sentence contains toxic language.\"\n",
        "    return f\"\"\"[INST] Rewrite the following sentence in one sentence to make it polite and non-toxic while keeping its meaning. The explanation highlights why the sentence is considered toxic:\n",
        "{toxic}\n",
        "Explanation: {explanation}\n",
        "[/INST]\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "zQCkAt28bJLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "\n",
        "def prompt_and_generate_with_explanation(toxic, explanation):\n",
        "    prompt = f\"\"\"[INST] Rewrite the following sentence in one sentence to make it polite and non-toxic while keeping its meaning. The explanation highlights why the sentence is considered toxic:\n",
        "{toxic}\n",
        "Explanation: {explanation}\n",
        "[/INST]\"\"\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        return_attention_mask=True\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_new_tokens=100,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # --- Clean output ---\n",
        "    # Remove prompt echo\n",
        "    decoded = decoded.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "    # Remove common prefixes\n",
        "    decoded = re.sub(\n",
        "        r\"(?i)^(rewritten|rephrased|revised|restated)?\\s*sentence\\s*[:\\-]?\\s*\", \"\", decoded\n",
        "    )\n",
        "    decoded = re.sub(r\"(?i)^(final answer|answer|note|output)[:\\-]?\\s*\", \"\", decoded)\n",
        "    decoded = re.sub(r\"(?i)^here('?s| is)?\\s+(a|the)?\\s*rewritten\\s+version\\s*(of (the )?sentence)?[:\\-]?\\s*\", \"\", decoded)\n",
        "    decoded = re.sub(r\"(?i)^##\\s*\", \"\", decoded)\n",
        "\n",
        "    # Extract only the first sentence\n",
        "    match = re.match(r\"^(.*?[.!?])(\\s|$)\", decoded)\n",
        "    detoxified = match.group(1).strip() if match else decoded.strip()\n",
        "\n",
        "    return detoxified"
      ],
      "metadata": {
        "id": "HrwyOjM6HJIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 6. Combined Prompt + Stage 1 Explanation with a fallback (This sentence contains toxic language.)"
      ],
      "metadata": {
        "id": "Uw5TfidcdM6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_and_generate_with_explanation(toxic_sentence):\n",
        "    stage1 = pipeline(toxic_sentence)\n",
        "    explanation = stage1.get(\"explanation\", \"This sentence contains toxic language.\")\n",
        "    prompt = build_llama_prompt_with_explanation(toxic_sentence, explanation)\n",
        "    return generate_response(prompt)\n"
      ],
      "metadata": {
        "id": "BbtC3lf1dKa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for one sentence :"
      ],
      "metadata": {
        "id": "9D9lsbWgqj-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "\n",
        "toxic = \"you are an idot.\"\n",
        "explanation = \"This sentence contains an insult.\"\n",
        "\n",
        "# ✅ Build prompt\n",
        "prompt = build_llama_prompt_with_explanation(toxic, explanation)\n",
        "\n",
        "# ✅ Tokenize with attention mask\n",
        "inputs = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=256,\n",
        "    return_attention_mask=True\n",
        ").to(\"cuda\")\n",
        "\n",
        "# ✅ Generate detoxified output\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=100,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# ✅ Remove prompt echo\n",
        "decoded = decoded.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "# ✅ Strip intro phrases\n",
        "decoded = re.sub(\n",
        "    r\"(?i)^(rewritten|rephrased|revised|restated)?\\s*sentence\\s*[:\\-]?\\s*\", \"\", decoded\n",
        ")\n",
        "decoded = re.sub(r\"(?i)^(final answer|answer|note|output)[:\\-]?\\s*\", \"\", decoded)\n",
        "decoded = re.sub(r\"(?i)^here('?s| is)?\\s+(a|the)?\\s*rewritten\\s+version\\s*(of (the )?sentence)?[:\\-]?\\s*\", \"\", decoded)\n",
        "decoded = re.sub(r\"(?i)^##\\s*\", \"\", decoded)\n",
        "\n",
        "# ✅ Extract just the first complete sentence\n",
        "match = re.match(r\"^(.*?[.!?])(\\s|$)\", decoded)\n",
        "detoxified = match.group(1).strip() if match else decoded.strip()\n",
        "\n",
        "print(\"Detoxified:\", detoxified)"
      ],
      "metadata": {
        "id": "SGw8LtfvAG43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for test set:"
      ],
      "metadata": {
        "id": "3HoEvGacqm5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")\n",
        "df[\"generated_output\"] = df[\"toxic_sentence\"].progress_apply(prompt_and_generate_with_explanation)\n",
        "\n",
        "df.to_csv(\"/content/drive/MyDrive/...\", index=False)\n",
        "print(\"Saved output with Stage 1 prompts.\")\n"
      ],
      "metadata": {
        "id": "-kWKN3GrdRZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/drive/MyDrive/...')"
      ],
      "metadata": {
        "id": "xxsmxIrtmLFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QnJzbvF2_NJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluation:"
      ],
      "metadata": {
        "id": "y_ZYMixid5lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")"
      ],
      "metadata": {
        "id": "6z5F76hpeInM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score evaluate sacrebleu\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "P, R, F1 = bert_score(\n",
        "    cands=df[\"generated_output\"].tolist(),\n",
        "    refs=df[\"neutral_reference\"].tolist(),\n",
        "    lang=\"en\"\n",
        ")\n",
        "\n",
        "print(f\"BERTScore F1: {F1.mean():.4f}\")"
      ],
      "metadata": {
        "id": "PEKU3jDqeInM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers sacrebleu"
      ],
      "metadata": {
        "id": "PJibvKlieInN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")\n",
        "\n",
        "# Rename columns to standard names for code consistency\n",
        "df.rename(columns={\n",
        "    \"toxic_sentence\": \"input\",\n",
        "    \"generated_output\": \"prediction\",\n",
        "    \"neutral_reference\": \"reference\"\n",
        "}, inplace=True)"
      ],
      "metadata": {
        "id": "GR7OWzGQeInO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_toxic(text):\n",
        "    result = pipeline(text)\n",
        "    return result[\"binary\"] == \"toxic\"\n",
        "\n",
        "df[\"STA\"] = [0 if is_toxic(pred) else 1 for pred in df[\"prediction\"]]"
      ],
      "metadata": {
        "id": "rixeSywteInQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load multilingual sentence similarity model\n",
        "labse = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
        "\n",
        "emb_input = labse.encode(df[\"input\"].tolist(), convert_to_tensor=True)\n",
        "emb_pred = labse.encode(df[\"prediction\"].tolist(), convert_to_tensor=True)\n",
        "\n",
        "similarities = util.cos_sim(emb_input, emb_pred).diagonal().tolist()\n",
        "df[\"SIM\"] = similarities"
      ],
      "metadata": {
        "id": "6HCT8hX1eInQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu.metrics import CHRF\n",
        "\n",
        "chrf = CHRF()\n",
        "chrf_scores = [\n",
        "    chrf.sentence_score(pred, [ref]).score / 100\n",
        "    for pred, ref in zip(df[\"prediction\"], df[\"reference\"])\n",
        "]\n",
        "df[\"CHRF\"] = chrf_scores"
      ],
      "metadata": {
        "id": "Lx9KDdQYeInQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Final J-score is the average of STA, SIM, and CHRF\n",
        "df[\"J-score\"] = (df[\"STA\"] + df[\"SIM\"] + df[\"CHRF\"]) / 3\n",
        "\n",
        "# Print final average\n",
        "print(f\"✅ J-score (mean over all examples): {df['J-score'].mean():.4f}\")"
      ],
      "metadata": {
        "id": "u2pniP3UeInR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"/content/...\", index=False)"
      ],
      "metadata": {
        "id": "0kkiE9HXeInR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/...')"
      ],
      "metadata": {
        "id": "SNHNOlxbeInS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Metric means from previously computed columns\n",
        "df[\"J-score\"] = (df[\"STA\"] + df[\"SIM\"] + df[\"CHRF\"]) / 3\n",
        "\n",
        "bert_f1 = F1.mean().item()                     # BERTScore F1\n",
        "chrf_score = df[\"CHRF\"].mean()                # CHRF\n",
        "sta_score = df[\"STA\"].mean()                  # STA\n",
        "sim_score = df[\"SIM\"].mean()                  # SIM\n",
        "j_score = df[\"J-score\"].mean()                # J-score\n",
        "\n",
        "# Calculate overall average metrics\n",
        "average_scores = {\n",
        "    \"Metric\": [\"BERTScore F1\", \"STA\", \"SIM\", \"CHRF\", \"Average J-score\"],\n",
        "    \"Score\": [\n",
        "        bert_f1,\n",
        "        sta_score,\n",
        "        sim_score,\n",
        "        chrf_score,\n",
        "        j_score\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame and export\n",
        "summary_df = pd.DataFrame(average_scores)\n",
        "summary_df.to_csv(\"/content/drive/MyDrive/...\", index=False)"
      ],
      "metadata": {
        "id": "pkUnr3MXeInT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/drive/MyDrive/...')"
      ],
      "metadata": {
        "id": "S-RJ0QkWeInT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}