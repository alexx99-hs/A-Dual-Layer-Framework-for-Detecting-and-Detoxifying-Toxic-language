{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0C8FOr7rf5A"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"...\")  # Replace with your real token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "xC6bhSXxr4ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load Mistral model\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_8bit=True\n",
        ")"
      ],
      "metadata": {
        "id": "-xsNWMPIr9C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "making a csv with just toxic and gold standard"
      ],
      "metadata": {
        "id": "vM7Jxlfn9J84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the file again (just to be safe)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")\n",
        "\n",
        "# Drop the mt0-base generation column\n",
        "df = df.drop(columns=[\"generated_output\"])\n",
        "\n",
        "# Save cleaned version (optional but good practice)\n",
        "df.to_csv(\"/content/drive/MyDrive/...\", index=False)\n",
        "\n",
        "print(\"Cleaned and saved:generated_outputs_cleaned.csv\")\n"
      ],
      "metadata": {
        "id": "LnL94ZsS8_Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")\n",
        "toxic_sentences = df[\"toxic_sentence\"].tolist()\n",
        "references = df[\"neutral_reference\"].tolist()\n"
      ],
      "metadata": {
        "id": "jq6RTxpOtXhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_simple_prompt(toxic_sentence):\n",
        "    return f\"\"\"[INST] Rewrite the following sentence to make it polite and non-toxic while keeping its meaning:\\n\"{toxic_sentence}\" [/INST]\"\"\"\n",
        "\n",
        "def generate_detoxified(prompt, model, tokenizer):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=60,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.5,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded.split(\"[/INST]\")[-1].strip()\n"
      ],
      "metadata": {
        "id": "6IH1J52B-CFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs_no_stage1 = []\n",
        "\n",
        "for text in toxic_sentences:\n",
        "    prompt = build_simple_prompt(text)\n",
        "    detoxified = generate_detoxified(prompt, model, tokenizer)\n",
        "    outputs_no_stage1.append(detoxified)\n",
        "\n",
        "df[\"mistral_no_stage1_output\"] = outputs_no_stage1\n",
        "df.to_csv(\"/content/drive/MyDrive/...\", index=False)\n",
        "\n",
        "print(\"Saved to: mistral_no_stage1_outputs.csv\")\n"
      ],
      "metadata": {
        "id": "jGaOjiOW-FNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluate Mistral detox outputs without Stage 1 using:\n",
        "\n",
        "✅ BERTScore: Semantic similarity to reference\n",
        "\n",
        "✅ CHRF: Surface n-gram overlap\n",
        "\n",
        "✅ STA: Style transfer accuracy using your Stage 1 classifier\n",
        "\n",
        "✅ J-score: Mean of the three"
      ],
      "metadata": {
        "id": "LMPerHm3_vUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bert_score sentence-transformers sacrebleu\n"
      ],
      "metadata": {
        "id": "AweXhaDR_uHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")\n",
        "print(df.columns)\n",
        "# Should include: 'toxic_sentence', 'neutral_reference', 'mistral_no_stage1_output'\n"
      ],
      "metadata": {
        "id": "gCyWKDuW_0JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bert_score sentence-transformers sacrebleu\n",
        "from bert_score import score\n",
        "\n",
        "P, R, F1 = score(\n",
        "    df[\"mistral_no_stage1_output\"].tolist(),\n",
        "    df[\"neutral_reference\"].tolist(),\n",
        "    lang=\"en\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "df[\"bertscore_f1\"] = F1.tolist()\n",
        "print(f\"Avg BERTScore F1: {F1.mean():.4f}\")\n"
      ],
      "metadata": {
        "id": "HIo916xh_7AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading classifier stage one for Compute CHRF + STA + J-score"
      ],
      "metadata": {
        "id": "IGWci1BCAGBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "class ToxicityClassifierPipeline:\n",
        "    def __init__(self, binary_model_path, fine_model_path, device=\"cuda\"):\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.tokenizer_binary = AutoTokenizer.from_pretrained(binary_model_path)\n",
        "        self.model_binary = AutoModelForSequenceClassification.from_pretrained(binary_model_path).to(self.device)\n",
        "\n",
        "        self.tokenizer_fine = AutoTokenizer.from_pretrained(fine_model_path)\n",
        "        self.model_fine = AutoModelForSequenceClassification.from_pretrained(fine_model_path).to(self.device)\n",
        "\n",
        "        self.label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "        self.label_to_explanation = {\n",
        "            \"toxic\": \"This sentence contains general toxic language.\",\n",
        "            \"severe_toxic\": \"This sentence includes severe verbal abuse.\",\n",
        "            \"obscene\": \"This sentence contains obscene or vulgar language.\",\n",
        "            \"threat\": \"This sentence includes a threat or incites violence.\",\n",
        "            \"insult\": \"This sentence includes personal insults.\",\n",
        "            \"identity_hate\": \"This sentence attacks a person's identity.\"\n",
        "        }\n",
        "\n",
        "    def __call__(self, comment, threshold=0.5):\n",
        "        # Binary prediction\n",
        "        inputs = self.tokenizer_binary(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_binary(**inputs)\n",
        "            probs = torch.softmax(outputs.logits, dim=1)\n",
        "            toxic_prob = probs[:, 1].item()\n",
        "\n",
        "        binary_pred = \"toxic\" if toxic_prob >= threshold else \"non-toxic\"\n",
        "\n",
        "        if binary_pred == \"non-toxic\":\n",
        "            return {\"binary\": binary_pred, \"explanation\": None, \"toxic_prob\": toxic_prob}\n",
        "\n",
        "        # Fine-grained labels\n",
        "        inputs = self.tokenizer_fine(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_fine(**inputs)\n",
        "            fine_probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]\n",
        "\n",
        "        # Generate full subtype score dictionary\n",
        "        subtypes = {label: float(prob) for label, prob in zip(self.label_cols, fine_probs)}\n",
        "\n",
        "        # Extract labels with score above threshold for explanation\n",
        "        active_labels = [\n",
        "            self.label_to_explanation[label]\n",
        "            for label, prob in zip(self.label_cols, fine_probs)\n",
        "            if prob >= threshold\n",
        "        ]\n",
        "\n",
        "        explanation = \" \".join(active_labels) if active_labels else \"This sentence was flagged as toxic.\"\n",
        "\n",
        "        return {\n",
        "            \"binary\": binary_pred,\n",
        "            \"subtypes\": subtypes,\n",
        "            \"toxic_prob\": toxic_prob,\n",
        "            \"explanation\": explanation\n",
        "        }\n"
      ],
      "metadata": {
        "id": "I-kjQTgELtYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust these paths to your actual model locations in Drive\n",
        "pipeline = ToxicityClassifierPipeline(\n",
        "    binary_model_path=\"/content/drive/MyDrive/...\",\n",
        "    fine_model_path=\"/content/drive/MyDrive/...\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "G2lkHq8sAXp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu.metrics import CHRF\n",
        "\n",
        "chrf = CHRF()\n",
        "sta = []\n",
        "chrf_scores = []\n",
        "\n",
        "for tox, detox, ref in zip(df[\"toxic_sentence\"], df[\"mistral_no_stage1_output\"], df[\"neutral_reference\"]):\n",
        "    # Style Transfer Accuracy\n",
        "    result = pipeline(detox)\n",
        "    sta.append(0 if result[\"binary\"] == \"toxic\" else 1)\n",
        "\n",
        "    # CHRF score\n",
        "    score_chrf = chrf.sentence_score(detox, [ref]).score / 100\n",
        "    chrf_scores.append(score_chrf)\n",
        "\n",
        "df[\"STA\"] = sta\n",
        "df[\"CHRF\"] = chrf_scores\n",
        "df[\"J_score\"] = (df[\"STA\"] + df[\"bertscore_f1\"] + df[\"CHRF\"]) / 3\n",
        "\n",
        "# Print average\n",
        "print(f\"Avg J-score: {df['J_score'].mean():.4f}\")\n",
        "df.to_csv(\"/content/drive/MyDrive/...\", index=False)\n",
        "print(\"Saved: mistral_no_stage1_evaluation.csv\")\n"
      ],
      "metadata": {
        "id": "M8KmX4zUAf45"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}