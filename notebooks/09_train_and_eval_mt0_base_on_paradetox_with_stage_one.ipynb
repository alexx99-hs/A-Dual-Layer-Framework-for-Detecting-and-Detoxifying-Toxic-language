{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1kxMVauXw-3"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "k4zFAz33Yezi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "\n",
        "class ToxicityClassifierPipeline:\n",
        "    def __init__(self, binary_model_path, fine_model_path, device=\"cuda\"):\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.tokenizer_binary = AutoTokenizer.from_pretrained(binary_model_path)\n",
        "        self.model_binary = AutoModelForSequenceClassification.from_pretrained(binary_model_path).to(self.device)\n",
        "\n",
        "        self.tokenizer_fine = AutoTokenizer.from_pretrained(fine_model_path)\n",
        "        self.model_fine = AutoModelForSequenceClassification.from_pretrained(fine_model_path).to(self.device)\n",
        "\n",
        "        self.label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "        self.label_to_explanation = {\n",
        "            \"toxic\": \"This sentence contains general toxic language.\",\n",
        "            \"severe_toxic\": \"This sentence includes severe verbal abuse.\",\n",
        "            \"obscene\": \"This sentence contains obscene or vulgar language.\",\n",
        "            \"threat\": \"This sentence includes a threat or incites violence.\",\n",
        "            \"insult\": \"This sentence includes personal insults.\",\n",
        "            \"identity_hate\": \"This sentence attacks a person's identity.\"\n",
        "        }\n",
        "\n",
        "    def __call__(self, comment, threshold=0.5):\n",
        "        inputs = self.tokenizer_binary(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_binary(**inputs)\n",
        "            probs = torch.softmax(outputs.logits, dim=1)\n",
        "            toxic_prob = probs[:, 1].item()\n",
        "\n",
        "        binary_pred = \"toxic\" if toxic_prob >= threshold else \"non-toxic\"\n",
        "\n",
        "        if binary_pred == \"non-toxic\":\n",
        "            return {\"binary\": binary_pred, \"explanation\": None}\n",
        "\n",
        "        inputs = self.tokenizer_fine(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_fine(**inputs)\n",
        "            fine_probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]\n",
        "\n",
        "        explanation_parts = [\n",
        "            self.label_to_explanation[label]\n",
        "            for label, prob in zip(self.label_cols, fine_probs)\n",
        "            if prob >= threshold\n",
        "        ]\n",
        "\n",
        "        explanation = \" \".join(explanation_parts) if explanation_parts else \"This sentence contains toxic language.\"\n",
        "        return {\n",
        "            \"binary\": binary_pred,\n",
        "            \"explanation\": explanation\n",
        "        }\n",
        "\n",
        "# Instantiate the classifier\n",
        "pipeline = ToxicityClassifierPipeline(\n",
        "    binary_model_path=\"/content/drive/MyDrive/...\",\n",
        "    fine_model_path=\"/content/drive/MyDrive/...\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "U0X2YiXxw9Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/...\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "model = MT5ForConditionalGeneration.from_pretrained(model_path, local_files_only=True).to(\"cuda\")\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully\")\n"
      ],
      "metadata": {
        "id": "El1U2rNCuq2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "orginal prompt: did'nt work good"
      ],
      "metadata": {
        "id": "Pfkfl-7JcjsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_mt0_prompt(toxic_sentence, explanation):\n",
        "    return (\n",
        "        f\"Detoxify the following sentence while preserving its meaning. \"\n",
        "        f\"The explanation below describes why the sentence is considered toxic:\\n\"\n",
        "        f\"Toxic: {toxic_sentence}\\n\"\n",
        "        f\"Explanation: {explanation}\\n\"\n",
        "        f\"Detoxified:\"\n",
        "    )\n",
        "\n",
        "def generate_mt0_detox(prompt, model, tokenizer):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=60,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2\n",
        "        )\n",
        "\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Remove prompt from output if echoed\n",
        "    if \"Detoxified:\" in decoded:\n",
        "        detoxified = decoded.split(\"Detoxified:\")[-1].strip()\n",
        "    else:\n",
        "        detoxified = decoded.strip()\n",
        "\n",
        "    return detoxified\n"
      ],
      "metadata": {
        "id": "8C4bKF3nZ1PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "aligned prompt format:"
      ],
      "metadata": {
        "id": "r2SPrgxjcx0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_stage1_aligned_prompt(toxic_sentence, stage1_labels):\n",
        "    \"\"\"\n",
        "    Aligned prompt structure: embeds Stage 1 tags compactly in the format the model was fine-tuned on.\n",
        "    \"\"\"\n",
        "    if stage1_labels:\n",
        "        tags = \", \".join(stage1_labels)\n",
        "        return f\"detoxify [{tags}]: {toxic_sentence}\"\n",
        "    else:\n",
        "        return f\"detoxify: {toxic_sentence}\"\n",
        "\n",
        "\n",
        "def generate_mt0_detox(prompt, model, tokenizer):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=60,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n"
      ],
      "metadata": {
        "id": "U8G5a2CIcogV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "orginal Generation Loop"
      ],
      "metadata": {
        "id": "9IiV153uc9lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test file\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")\n",
        "\n",
        "# Generate new detox outputs\n",
        "outputs_mt0_stage1 = []\n",
        "\n",
        "for text in df[\"toxic_sentence\"]:\n",
        "    stage1 = pipeline(text)\n",
        "    explanation = stage1[\"explanation\"] if stage1[\"explanation\"] else \"This sentence contains toxic language.\"\n",
        "    prompt = build_mt0_prompt(text, explanation)\n",
        "    # Changed mt0_model to model and mt0_tokenizer to tokenizer\n",
        "    detox = generate_mt0_detox(prompt, model, tokenizer)\n",
        "    outputs_mt0_stage1.append(detox)\n",
        "\n",
        "# Save new outputs\n",
        "df[\"mt0_base_output_stage1\"] = outputs_mt0_stage1\n",
        "df.to_csv(\"/content/drive/MyDrive/...\", index=False)\n",
        "\n",
        "print(\"Saved to: mt0_base_retrain_output_with_stage1_vol2.csv\")\n"
      ],
      "metadata": {
        "id": "hIWoZHpQxboc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update the Generation Loop"
      ],
      "metadata": {
        "id": "gMveX_XUdB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test file\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")\n",
        "\n",
        "# Generate new detox outputs\n",
        "outputs_mt0_stage1 = []\n",
        "\n",
        "for text in df[\"toxic_sentence\"]:\n",
        "    stage1 = pipeline(text)\n",
        "    explanation = stage1[\"explanation\"]\n",
        "    tags = []\n",
        "\n",
        "    if explanation:\n",
        "        # Only extract tags if explanation exists\n",
        "        for label, phrase in pipeline.label_to_explanation.items():\n",
        "            if phrase in explanation:\n",
        "                tags.append(label)\n",
        "\n",
        "    prompt = build_stage1_aligned_prompt(text, tags)\n",
        "    detox = generate_mt0_detox(prompt, model, tokenizer)\n",
        "    outputs_mt0_stage1.append(detox)\n",
        "\n",
        "\n",
        "\n",
        "# Save new outputs\n",
        "df[\"mt0_base_output_stage1\"] = outputs_mt0_stage1\n",
        "df.to_csv(\"/content/drive/MyDrive/...\", index=False)\n",
        "print(\"Saved with aligned prompt format.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "x36K7mrodDmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/drive/MyDrive/...')\n"
      ],
      "metadata": {
        "id": "ZkLrcS-ahTaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified Version of  Generation Loop only for submission in leaderboard PAN 2024"
      ],
      "metadata": {
        "id": "y_DiQp0Agpj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the official test set\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\", sep=\"\\t\")\n",
        "\n",
        "# Initialize output list\n",
        "neutral_sentences = []\n",
        "\n",
        "# Loop through rows\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    toxic = row['toxic_sentence']\n",
        "    lang = row['lang']\n",
        "\n",
        "    if lang == 'en':\n",
        "        # Run stage 1 classifier\n",
        "        stage1 = pipeline(toxic)\n",
        "        explanation = stage1[\"explanation\"]\n",
        "        tags = []\n",
        "\n",
        "        if explanation:\n",
        "            for label, phrase in pipeline.label_to_explanation.items():\n",
        "                if phrase in explanation:\n",
        "                    tags.append(label)\n",
        "\n",
        "        # Build aligned prompt + generate detoxified sentence\n",
        "        prompt = build_stage1_aligned_prompt(toxic, tags)\n",
        "        detox = generate_mt0_detox(prompt, model, tokenizer)\n",
        "        neutral_sentences.append(detox)\n",
        "    else:\n",
        "        # For all other languages, keep original toxic\n",
        "        neutral_sentences.append(toxic)\n",
        "\n",
        "# Write predictions back to the dataframe\n",
        "df['neutral_sentence'] = neutral_sentences\n",
        "\n",
        "# Save as official TSV\n",
        "output_path = \"/content/drive/MyDrive/...\"\n",
        "df.to_csv(output_path, sep=\"\\t\", index=False)\n",
        "\n",
        "print(f\"Saved final submission to {output_path}\")\n"
      ],
      "metadata": {
        "id": "iJTIz-nqgpNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip submission.zip /content/drive/MyDrive/submission.tsv\n"
      ],
      "metadata": {
        "id": "-uGqFtEPj3mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/drive/MyDrive/...')"
      ],
      "metadata": {
        "id": "jS9EmCx0ilGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"submission.zip\")\n"
      ],
      "metadata": {
        "id": "SJh7ZI4yj5Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load current version\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\", sep=\"\\t\")\n",
        "\n",
        "# Keep only required columns\n",
        "df_clean = df[['toxic_sentence', 'neutral_sentence']]\n",
        "\n",
        "# Save cleaned version\n",
        "df_clean.to_csv(\"submission_clean.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "# Zip it\n",
        "!zip submission.zip submission_clean.tsv\n"
      ],
      "metadata": {
        "id": "23xJZpGLtyXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"submission_clean.tsv\")\n"
      ],
      "metadata": {
        "id": "gpJRrx9huEbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified Version of Generation Loop only for submission in leaderboard PAN 2025"
      ],
      "metadata": {
        "id": "XoNy1hBQyFt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the new PAN 2025 test file\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/thesis files/...\", sep=\"\\t\")\n",
        "\n",
        "# Initialize output list\n",
        "neutral_sentences = []\n",
        "\n",
        "# Loop through all rows\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    toxic = row['toxic_sentence']\n",
        "    lang = row['lang']\n",
        "\n",
        "    if lang == 'en':\n",
        "        # Run stage 1 classifier\n",
        "        stage1 = pipeline(toxic)\n",
        "        explanation = stage1.get(\"explanation\", \"\")\n",
        "        tags = []\n",
        "\n",
        "        if explanation:\n",
        "            for label, phrase in pipeline.label_to_explanation.items():\n",
        "                if phrase in explanation:\n",
        "                    tags.append(label)\n",
        "\n",
        "        # Build prompt + detoxify\n",
        "        prompt = build_stage1_aligned_prompt(toxic, tags)\n",
        "        detox = generate_mt0_detox(prompt, model, tokenizer)\n",
        "        neutral_sentences.append(detox)\n",
        "    else:\n",
        "        # For other languages, keep the toxic sentence\n",
        "        neutral_sentences.append(toxic)\n",
        "\n",
        "# Assign back to DataFrame\n",
        "df['neutral_sentence'] = neutral_sentences\n",
        "\n",
        "# Save submission file\n",
        "df = df[['toxic_sentence', 'neutral_sentence', 'lang']]\n",
        "df.to_csv(\"/content/drive/MyDrive/...\", sep=\"\\t\", index=False)\n",
        "\n",
        "# Zip for Codalab submission\n",
        "!zip /content/drive/MyDrive/... /content/drive/MyDrive/...\n",
        "\n",
        "print(\"Submission file saved and zipped for PAN 2025.\")\n"
      ],
      "metadata": {
        "id": "Gu0e60JhyMMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"submission_clean.tsv\")"
      ],
      "metadata": {
        "id": "KADYultm1ZrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluation:"
      ],
      "metadata": {
        "id": "Vf0mlN-VgZTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R3PsRiJu1U3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")"
      ],
      "metadata": {
        "id": "t7S_QPtYga-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score evaluate sacrebleu\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "P, R, F1 = bert_score(\n",
        "    cands=df[\"mt0_base_output_stage1\"].tolist(),\n",
        "    refs=df[\"neutral_reference\"].tolist(),\n",
        "    lang=\"en\"\n",
        ")\n",
        "\n",
        "print(f\"BERTScore F1: {F1.mean():.4f}\")"
      ],
      "metadata": {
        "id": "sGRyW7jwh3Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers sacrebleu"
      ],
      "metadata": {
        "id": "v_5HUTIPiToz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/....\")  # adjust path if needed\n",
        "\n",
        "# Rename columns to standard names for code consistency\n",
        "df.rename(columns={\n",
        "    \"toxic_sentence\": \"input\",\n",
        "    \"mt0_base_output_stage1\": \"prediction\",\n",
        "    \"neutral_reference\": \"reference\"\n",
        "}, inplace=True)"
      ],
      "metadata": {
        "id": "lWWHD18ViWiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "class ToxicityClassifierPipeline:\n",
        "    def __init__(self, binary_model_path, fine_model_path, device=\"cuda\"):\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.tokenizer_binary = AutoTokenizer.from_pretrained(binary_model_path)\n",
        "        self.model_binary = AutoModelForSequenceClassification.from_pretrained(binary_model_path).to(self.device)\n",
        "\n",
        "        self.tokenizer_fine = AutoTokenizer.from_pretrained(fine_model_path)\n",
        "        self.model_fine = AutoModelForSequenceClassification.from_pretrained(fine_model_path).to(self.device)\n",
        "\n",
        "        self.label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "        self.label_to_explanation = {\n",
        "            \"toxic\": \"This sentence contains general toxic language.\",\n",
        "            \"severe_toxic\": \"This sentence contains extreme hostility or verbal abuse.\",\n",
        "            \"obscene\": \"This sentence contains obscene or vulgar language.\",\n",
        "            \"threat\": \"This sentence contains a threat or implied violence.\",\n",
        "            \"insult\": \"This sentence includes personal insults or demeaning language.\",\n",
        "            \"identity_hate\": \"This sentence attacks someone based on identity (e.g. race, gender, religion).\"\n",
        "        }\n",
        "\n",
        "    def __call__(self, comment, threshold=0.5):\n",
        "        inputs = self.tokenizer_binary(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_binary(**inputs)\n",
        "            probs = torch.softmax(outputs.logits, dim=1)\n",
        "            toxic_prob = probs[:, 1].item()\n",
        "\n",
        "        binary_pred = \"toxic\" if toxic_prob >= threshold else \"non-toxic\"\n",
        "\n",
        "        if binary_pred == \"non-toxic\":\n",
        "            return {\"binary\": binary_pred, \"subtypes\": None, \"toxic_prob\": toxic_prob, \"explanation\": None}\n",
        "\n",
        "        inputs = self.tokenizer_fine(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_fine(**inputs)\n",
        "            fine_probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]\n",
        "\n",
        "        subtypes = {\n",
        "            label: round(float(prob), 2)\n",
        "            for label, prob in zip(self.label_cols, fine_probs)\n",
        "            if prob >= threshold\n",
        "        }\n",
        "\n",
        "        explanation_parts = [self.label_to_explanation[label] for label in subtypes]\n",
        "        explanation = \" \".join(explanation_parts) if explanation_parts else None\n",
        "\n",
        "        return {\n",
        "            \"binary\": binary_pred,\n",
        "            \"subtypes\": subtypes,\n",
        "            \"toxic_prob\": toxic_prob,\n",
        "            \"explanation\": explanation\n",
        "        }"
      ],
      "metadata": {
        "id": "XALyrp5tig-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = ToxicityClassifierPipeline(\n",
        "    binary_model_path=\"/content/drive/MyDrive/...\",\n",
        "    fine_model_path=\"/content/drive/MyDrive/...\"\n",
        ")"
      ],
      "metadata": {
        "id": "zu2FeblJiluk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_toxic(text):\n",
        "    result = pipeline(text)\n",
        "    return result[\"binary\"] == \"toxic\"\n",
        "\n",
        "df[\"STA\"] = [0 if is_toxic(pred) else 1 for pred in df[\"prediction\"]]"
      ],
      "metadata": {
        "id": "hYq5FzitioGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load multilingual sentence similarity model\n",
        "labse = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
        "\n",
        "emb_input = labse.encode(df[\"input\"].tolist(), convert_to_tensor=True)\n",
        "emb_pred = labse.encode(df[\"prediction\"].tolist(), convert_to_tensor=True)\n",
        "\n",
        "similarities = util.cos_sim(emb_input, emb_pred).diagonal().tolist()\n",
        "df[\"SIM\"] = similarities"
      ],
      "metadata": {
        "id": "_mh_Cd0Gip7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu.metrics import CHRF\n",
        "\n",
        "chrf = CHRF()\n",
        "chrf_scores = [\n",
        "    chrf.sentence_score(pred, [ref]).score / 100\n",
        "    for pred, ref in zip(df[\"prediction\"], df[\"reference\"])\n",
        "]\n",
        "df[\"CHRF\"] = chrf_scores"
      ],
      "metadata": {
        "id": "FOaBNXK3itih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Final J-score is the average of STA, SIM, and CHRF\n",
        "df[\"J-score\"] = (df[\"STA\"] + df[\"SIM\"] + df[\"CHRF\"]) / 3\n",
        "\n",
        "# Print final average\n",
        "print(f\"✅ J-score (mean over all examples): {df['J-score'].mean():.4f}\")"
      ],
      "metadata": {
        "id": "79FYJ01KixUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"/content/...\", index=False)"
      ],
      "metadata": {
        "id": "P7xwtrK0i0JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/...')"
      ],
      "metadata": {
        "id": "aX6y6SEai4fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Metric means from previously computed columns\n",
        "df[\"J-score\"] = (df[\"STA\"] + df[\"SIM\"] + df[\"CHRF\"]) / 3\n",
        "\n",
        "bert_f1 = F1.mean().item()                     # BERTScore F1\n",
        "chrf_score = df[\"CHRF\"].mean()                # CHRF\n",
        "sta_score = df[\"STA\"].mean()                  # STA\n",
        "sim_score = df[\"SIM\"].mean()                  # SIM\n",
        "j_score = df[\"J-score\"].mean()                # J-score\n",
        "\n",
        "# Calculate overall average metrics\n",
        "average_scores = {\n",
        "    \"Metric\": [\"BERTScore F1\", \"STA\", \"SIM\", \"CHRF\", \"Average J-score\"],\n",
        "    \"Score\": [\n",
        "        bert_f1,\n",
        "        sta_score,\n",
        "        sim_score,\n",
        "        chrf_score,\n",
        "        j_score\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame and export\n",
        "summary_df = pd.DataFrame(average_scores)\n",
        "summary_df.to_csv(\"/content/drive/MyDrive/...\", index=False)"
      ],
      "metadata": {
        "id": "4ajdZk6LjwG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/drive/MyDrive/...\")"
      ],
      "metadata": {
        "id": "-d2SsDPPjz9R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}