{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "YkYnuFtuRX-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gurobipy[matrixapi] \"numpy<2\""
      ],
      "metadata": {
        "id": "9IoHda3RUK4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8KG22jSPbq4"
      },
      "outputs": [],
      "source": [
        "# Step 1: Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import hamming_loss, classification_report\n",
        "\n",
        "# Step 2: Load saved fine-grained model\n",
        "model_path = \"/content/drive/MyDrive/...\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load and filter Jigsaw dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")\n",
        "label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "df[label_cols] = df[label_cols].fillna(0).astype(int)\n",
        "df[\"toxic_binary\"] = df[label_cols].max(axis=1)\n",
        "df_fine = df[df[\"toxic_binary\"] == 1][[\"comment_text\"] + label_cols]\n",
        "\n",
        "# Step 4: Split + Save splits\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, eval_df = train_test_split(df_fine, test_size=0.1, random_state=42)\n",
        "train_df.to_csv(\"/content/drive/MyDrive/train...\", index=False)\n",
        "eval_df.to_csv(\"/content/drive/MyDrive/eval...\", index=False)"
      ],
      "metadata": {
        "id": "R9SuIzTgR63Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Convert eval split to HuggingFace Dataset\n",
        "eval_fine = Dataset.from_pandas(eval_df)\n",
        "\n",
        "# Step 6: Tokenization\n",
        "def tokenize(example):\n",
        "    tokens = tokenizer(example[\"comment_text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
        "    tokens[\"labels\"] = [example[label] for label in label_cols]\n",
        "    return tokens\n",
        "\n",
        "eval_fine = eval_fine.map(tokenize)\n",
        "eval_fine = eval_fine.with_format(\"python\")\n",
        "eval_fine = eval_fine.map(lambda x: {\"labels\": np.array(x[\"labels\"], dtype=np.float32)})\n",
        "eval_fine.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
      ],
      "metadata": {
        "id": "fFiZ_YizR7rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Custom DataLoader\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        \"input_ids\": torch.stack([x[\"input_ids\"] for x in batch]),\n",
        "        \"attention_mask\": torch.stack([x[\"attention_mask\"] for x in batch]),\n",
        "        \"labels\": torch.stack([torch.tensor(np.asarray(x[\"labels\"], dtype=np.float32)) for x in batch])\n",
        "    }\n",
        "\n",
        "eval_loader = DataLoader(eval_fine, batch_size=32, collate_fn=collate_fn)\n",
        "\n"
      ],
      "metadata": {
        "id": "S3Vy7Zr3SGYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Predict\n",
        "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
        "def binarize(logits, threshold=0.5): return (sigmoid(logits) > threshold).astype(int)\n",
        "\n",
        "model.eval()\n",
        "all_logits, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in eval_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
        "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
        "        labels = batch[\"labels\"].cpu().numpy()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits.cpu().numpy()\n",
        "\n",
        "        all_logits.append(logits)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "logits = np.vstack(all_logits)\n",
        "true_labels = np.vstack(all_labels)\n",
        "pred_labels = binarize(logits)"
      ],
      "metadata": {
        "id": "LNWki9SmTy9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Save predictions to CSV\n",
        "output_df = pd.DataFrame({\n",
        "    \"comment_text\": eval_df[\"comment_text\"].values\n",
        "})\n",
        "for i, col in enumerate(label_cols):\n",
        "    output_df[f\"true_{col}\"] = true_labels[:, i]\n",
        "    output_df[f\"pred_{col}\"] = pred_labels[:, i]\n",
        "\n",
        "output_df.to_csv(\"/content/drive/MyDrive/...\", index=False)\n",
        "print(\"Predictions saved to Drive\")\n",
        "\n",
        "# Step 10: Evaluation\n",
        "hloss = hamming_loss(true_labels, pred_labels)\n",
        "print(f\"Hamming Loss: {hloss:.4f}\")\n",
        "\n",
        "report = classification_report(true_labels, pred_labels, target_names=label_cols, digits=4, zero_division=0)\n",
        "print(report)\n",
        "\n",
        "#save report\n",
        "#the 29th one is the last one\n",
        "with open(\"/content/drive/MyDrive/...\", \"w\") as f:\n",
        "    f.write(report)\n",
        "print(\"Classification report saved.\")"
      ],
      "metadata": {
        "id": "XJaV7LFgU4fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Get confusion matrices for each label\n",
        "cm_per_label = multilabel_confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "# Plot each one\n",
        "for i, label in enumerate(label_cols):\n",
        "    cm = cm_per_label[i]\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Not \" + label, label], yticklabels=[\"Not \" + label, label])\n",
        "    plt.title(f\"Confusion Matrix: {label}\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "MBHu9w1MW83K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show Label Distribution in Train and Test Splits"
      ],
      "metadata": {
        "id": "3WPPseBcYR_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ðŸ”¹ Label distribution in TRAIN split:\")\n",
        "print(train_df[label_cols].sum().sort_values(ascending=False))\n",
        "\n",
        "print(\"\\nðŸ”¹ Label distribution in TEST split:\")\n",
        "print(eval_df[label_cols].sum().sort_values(ascending=False))\n"
      ],
      "metadata": {
        "id": "9QWkNO9AYTt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ðŸ”¹ Relative label % in TRAIN split:\")\n",
        "print((train_df[label_cols].sum() / len(train_df)).sort_values(ascending=False).apply(lambda x: f\"{x:.2%}\"))\n",
        "\n",
        "print(\"\\nðŸ”¹ Relative label % in TEST split:\")\n",
        "print((eval_df[label_cols].sum() / len(eval_df)).sort_values(ascending=False).apply(lambda x: f\"{x:.2%}\"))\n"
      ],
      "metadata": {
        "id": "nX5R545VYYlr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}