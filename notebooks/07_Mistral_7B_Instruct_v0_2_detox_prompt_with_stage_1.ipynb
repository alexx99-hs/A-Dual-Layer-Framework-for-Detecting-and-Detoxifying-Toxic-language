{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"...\")  # Replace with your real token\n"
      ],
      "metadata": {
        "id": "4zul3m4HDdJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "4Ue7p2h6FpRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4av174yBUIg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_8bit=True  # Optional: Saves memory on Colab\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(toxic_sentence):\n",
        "    return f\"<s>[INST] Rewrite the following sentence in 1 short sentence to make it polite and non-toxic while keeping its meaning:\\n\\\"{toxic_sentence}\\\" [/INST]\"\n",
        "\n",
        "def detoxify(text):\n",
        "    prompt = format_prompt(text)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    detoxified = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return detoxified\n"
      ],
      "metadata": {
        "id": "IUWYrhzKC4nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toxic_examples = [\n",
        "    \"You're such a pathetic loser.\",\n",
        "    \"No one cares about your stupid opinions.\",\n",
        "    \"Shut up, you're a worthless idiot.\"\n",
        "]\n",
        "\n",
        "for i, text in enumerate(toxic_examples):\n",
        "    print(f\"\\n{i+1}. Original: {text}\")\n",
        "    print(f\"   Detoxified: {detoxify(text)}\")\n"
      ],
      "metadata": {
        "id": "S-BXMKwaC5zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toxic_examples = [\n",
        "    \"don't do abortion close your legs.\"\n",
        "]\n",
        "\n",
        "for i, text in enumerate(toxic_examples):\n",
        "    print(f\"\\n{i+1}. Original: {text}\")\n",
        "    print(f\"   Detoxified: {detoxify(text)}\")"
      ],
      "metadata": {
        "id": "crgP8QwuG7En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full Pipeline: Stage 1 + Prompting Mistral"
      ],
      "metadata": {
        "id": "B00neTbQLcV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate bitsandbytes\n"
      ],
      "metadata": {
        "id": "il7iBPBjLgJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Step 2: Import Libraries and Load Stage 1 Classifier"
      ],
      "metadata": {
        "id": "ACnW7pjXLsSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "class ToxicityClassifierPipeline:\n",
        "    def __init__(self, binary_model_path, fine_model_path, device=\"cuda\"):\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.tokenizer_binary = AutoTokenizer.from_pretrained(binary_model_path)\n",
        "        self.model_binary = AutoModelForSequenceClassification.from_pretrained(binary_model_path).to(self.device)\n",
        "\n",
        "        self.tokenizer_fine = AutoTokenizer.from_pretrained(fine_model_path)\n",
        "        self.model_fine = AutoModelForSequenceClassification.from_pretrained(fine_model_path).to(self.device)\n",
        "\n",
        "        self.label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "        self.label_to_explanation = {\n",
        "            \"toxic\": \"This sentence contains general toxic language.\",\n",
        "            \"severe_toxic\": \"This sentence includes severe verbal abuse.\",\n",
        "            \"obscene\": \"This sentence contains obscene or vulgar language.\",\n",
        "            \"threat\": \"This sentence includes a threat or incites violence.\",\n",
        "            \"insult\": \"This sentence includes personal insults.\",\n",
        "            \"identity_hate\": \"This sentence attacks a person's identity.\"\n",
        "        }\n",
        "\n",
        "    def __call__(self, comment, threshold=0.5):\n",
        "        # Binary prediction\n",
        "        inputs = self.tokenizer_binary(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_binary(**inputs)\n",
        "            probs = torch.softmax(outputs.logits, dim=1)\n",
        "            toxic_prob = probs[:, 1].item()\n",
        "\n",
        "        binary_pred = \"toxic\" if toxic_prob >= threshold else \"non-toxic\"\n",
        "\n",
        "        if binary_pred == \"non-toxic\":\n",
        "            return {\"binary\": binary_pred, \"explanation\": None, \"toxic_prob\": toxic_prob}\n",
        "\n",
        "        # Fine-grained labels\n",
        "        inputs = self.tokenizer_fine(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_fine(**inputs)\n",
        "            fine_probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]\n",
        "\n",
        "        # ‚úÖ Generate full subtype score dictionary\n",
        "        subtypes = {label: float(prob) for label, prob in zip(self.label_cols, fine_probs)}\n",
        "\n",
        "        # Extract labels with score above threshold for explanation\n",
        "        active_labels = [\n",
        "            self.label_to_explanation[label]\n",
        "            for label, prob in zip(self.label_cols, fine_probs)\n",
        "            if prob >= threshold\n",
        "        ]\n",
        "\n",
        "        explanation = \" \".join(active_labels) if active_labels else \"This sentence was flagged as toxic.\"\n",
        "\n",
        "        return {\n",
        "            \"binary\": binary_pred,\n",
        "            \"subtypes\": subtypes,\n",
        "            \"toxic_prob\": toxic_prob,\n",
        "            \"explanation\": explanation\n",
        "        }\n"
      ],
      "metadata": {
        "id": "I-kjQTgELtYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Step 3: Load Stage 1 Model"
      ],
      "metadata": {
        "id": "av2aquLoLvjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "s7r-w6tdN4CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust these paths to your actual model locations in Drive\n",
        "pipeline = ToxicityClassifierPipeline(\n",
        "    binary_model_path=\"/content/drive/MyDrive/...\",\n",
        "    fine_model_path=\"/content/drive/MyDrive/...\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "ly-ng5KPLySS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Step 4: Load Mistral-7B-Instruct for Detoxification"
      ],
      "metadata": {
        "id": "o09lPjxZLz6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_8bit=True\n",
        ")\n",
        "\n",
        "mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n"
      ],
      "metadata": {
        "id": "r91YdSVrL2jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Step 5: Prompt Function + Generation"
      ],
      "metadata": {
        "id": "43PFlMEAL48e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(toxic_sentence, explanation):\n",
        "    return (\n",
        "        f\"[INST] Rewrite the following sentence to make it polite and non-toxic while keeping its meaning. \"\n",
        "        f\"Keep your response short and efficient ‚Äî one sentence only. \"\n",
        "        f\"The explanation below shows why the sentence is considered toxic:\\n\"\n",
        "        f\"{toxic_sentence}\\n\"\n",
        "        f\"Explanation: {explanation}\\n\"\n",
        "        f\"[/INST]\"\n",
        "    )\n",
        "\n",
        "def generate_detoxified(prompt, model, tokenizer):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=40,              #  tighter output\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.5,         # stronger constraint\n",
        "            #no_repeat_ngram_size=3,         #  block repeated phrases\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "# Keep only what's after [/INST]\n",
        "    detoxified = raw_output.split(\"[/INST]\")[-1].strip()\n",
        "    return detoxified\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J5wjEmG_L7jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Step 6: Run on a Sample Toxic Sentence"
      ],
      "metadata": {
        "id": "jo1UcS1PL9LB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toxic_sentence = \"fuck you.\"\n",
        "result = pipeline(toxic_sentence)\n",
        "\n",
        "if result[\"binary\"] == \"toxic\":\n",
        "    prompt = build_prompt(toxic_sentence, result[\"explanation\"])\n",
        "    detoxified = generate_detoxified(prompt, mistral_model, mistral_tokenizer)\n",
        "\n",
        "    print(\"üß™ Stage 1 Classification:\")\n",
        "    print(f\"Binary: {result['binary']}\")\n",
        "    print(\"Subtype scores:\")\n",
        "    if \"subtypes\" in result and result[\"subtypes\"]:\n",
        "       for label, score in result[\"subtypes\"].items():\n",
        "            print(f\"  {label}: {score}\")\n",
        "    else:\n",
        "        print(\"  None\")\n",
        "\n",
        "    print(f\"\\nExplanation: {result['explanation']}\")\n",
        "    #print(\"\\nüìù Prompt:\")\n",
        "    #print(prompt)\n",
        "    print(\"\\n‚úÖ Detoxified Output:\")\n",
        "    print(detoxified)\n",
        "else:\n",
        "    print(\"Sentence is not toxic ‚Äî no detox needed.\")\n"
      ],
      "metadata": {
        "id": "PEmhJ6GhL_wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "runing on sets:::"
      ],
      "metadata": {
        "id": "3vCgLJTkesoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load toxic test set\n",
        "df = pd.read_csv(\"/content/...\")\n",
        "\n",
        "# Add result columns\n",
        "results = []\n",
        "\n",
        "for sentence in df[\"toxic_sentence\"]:\n",
        "    classification = pipeline(sentence)\n",
        "\n",
        "    if classification[\"binary\"] == \"toxic\":\n",
        "        prompt = build_prompt(sentence, classification[\"explanation\"])\n",
        "        detoxified = generate_detoxified(prompt, mistral_model, mistral_tokenizer)\n",
        "    else:\n",
        "        detoxified = sentence  # Keep unchanged if not toxic\n",
        "\n",
        "    # Add row\n",
        "    results.append({\n",
        "        \"toxic_sentence\": sentence,\n",
        "        \"binary_label\": classification[\"binary\"],\n",
        "        \"subtypes\": classification.get(\"subtypes\", {}),\n",
        "        \"explanation\": classification[\"explanation\"],\n",
        "        \"detoxified_output\": detoxified\n",
        "    })\n",
        "\n",
        "# Convert to dataframe\n",
        "df_out = pd.DataFrame(results)\n",
        "df_out.to_csv(\"/content/...\", index=False)\n",
        "\n",
        "print(\"Results saved to /content/...\")\n"
      ],
      "metadata": {
        "id": "un8SA-I3eu_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/...\")\n"
      ],
      "metadata": {
        "id": "1d8Z7YAxe0nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Plan: Run Full System on ParadeTox Test Set\n",
        "We‚Äôll do:\n",
        "\n",
        "‚úÖ Load the official test set\n",
        "\n",
        "‚úÖ Run each toxic sentence through Stage 1 (classifier)\n",
        "\n",
        "If toxic, create a prompt with explanation ‚Üí run Mistral\n",
        "\n",
        "If non-toxic, keep original\n",
        "\n",
        "‚úÖ Save everything (input, explanation, output) to CSV\n",
        "\n",
        "‚úÖ Evaluate scores (BERTScore, J-score, toxicity, etc.)\n",
        "\n"
      ],
      "metadata": {
        "id": "zA53xWlOm-EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")\n",
        "\n",
        "# Prepare output column\n",
        "mistral_outputs = []\n",
        "\n",
        "# Loop through toxic sentences and generate detoxified versions using your Mistral+Stage1 setup\n",
        "for i, row in df.iterrows():\n",
        "    toxic = row[\"toxic_sentence\"]\n",
        "\n",
        "    # Step 1: Get classification + explanation\n",
        "    stage1 = pipeline(toxic)\n",
        "    explanation = stage1[\"explanation\"] if stage1[\"explanation\"] else \"No explanation available.\"\n",
        "\n",
        "    # Build the prompt (same as before)\n",
        "    prompt = f\"\"\"[INST] Rewrite the following sentence in one sentence to make it polite and non-toxic while keeping its meaning. The explanation highlights why the sentence is considered toxic:\n",
        "{toxic}\n",
        "Explanation: {explanation}\n",
        "[/INST]\"\"\"\n",
        "\n",
        "    # Step 2: Run Mistral generation\n",
        "    output = generate_detoxified(prompt, mistral_model, mistral_tokenizer)\n",
        "    detoxified = output.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "    mistral_outputs.append(detoxified)\n",
        "\n",
        "# Save results\n",
        "df[\"mistral_output\"] = mistral_outputs\n",
        "df.to_csv(\"/content/drive/MyDrive/...\", index=False)\n",
        "print(\"Done. Saved as mistral_generated_outputs_testset.csv\")\n"
      ],
      "metadata": {
        "id": "FxJWNGbDaLMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluate the Mistral-generated outputs using:\n",
        "\n",
        "‚úÖ BERTScore ‚Äì semantic similarity to gold standard (neutral_reference)\n",
        "\n",
        "‚úÖ J-score ‚Äì combines style transfer accuracy, semantic similarity, and CHRF"
      ],
      "metadata": {
        "id": "qkMsAZNhkfaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bert_score sentence-transformers sacrebleu\n"
      ],
      "metadata": {
        "id": "lZ9RiQBhkZXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")\n",
        "toxic_sentences = df[\"toxic_sentence\"].tolist()\n",
        "references = df[\"neutral_reference\"].tolist()\n",
        "\n"
      ],
      "metadata": {
        "id": "h7Xvp9B2keWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import score\n",
        "\n",
        "# Compute BERTScore (F1 score)\n",
        "P, R, F1 = score(df[\"mistral_output\"].tolist(), df[\"neutral_reference\"].tolist(), lang=\"en\", verbose=True)\n",
        "\n",
        "# Store scores in DataFrame\n",
        "df[\"bertscore_f1\"] = F1.tolist()\n",
        "print(f\"Avg BERTScore F1: {F1.mean():.4f}\")\n"
      ],
      "metadata": {
        "id": "zdOEFqs5knJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu.metrics import CHRF\n",
        "\n",
        "chrf = CHRF()\n",
        "sta = []  # style transfer accuracy\n",
        "chrf_scores = []\n",
        "for toxic, detox in zip(df[\"toxic_sentence\"], df[\"mistral_output\"]):\n",
        "    # Check if detox is non-toxic using your classifier\n",
        "    result = pipeline(detox)\n",
        "    sta.append(0 if result[\"binary\"] == \"toxic\" else 1)\n",
        "\n",
        "    # Compute CHRF\n",
        "    score_chrf = chrf.sentence_score(detox, [df[\"neutral_reference\"].iloc[len(sta)-1]]).score / 100\n",
        "    chrf_scores.append(score_chrf)\n",
        "\n",
        "df[\"STA\"] = sta\n",
        "df[\"CHRF\"] = chrf_scores\n",
        "df[\"J_score\"] = (df[\"STA\"] + df[\"bertscore_f1\"] + df[\"CHRF\"]) / 3\n",
        "\n",
        "# Final score\n",
        "print(f\"Avg J-score: {df['J_score'].mean():.4f}\")\n"
      ],
      "metadata": {
        "id": "3vdKSd5gmB3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"/content/drive/MyDrive/...\", index=False)\n",
        "print(\"Saved: mistral_evaluation_with_scores.csv\")\n"
      ],
      "metadata": {
        "id": "UhGM1LfPmGTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YZVNpfndrWKL"
      }
    }
  ]
}