{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mpVL0-H_Xji"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"textdetox/multilingual_paradetox_test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "YxLy9QYDAfee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = dataset[\"en\"]\n",
        "print(test_data[0])  # Print the first example\n"
      ],
      "metadata": {
        "id": "3eh4SVGcA0bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.save_to_disk(\"multilingual_paradetox_test\")\n"
      ],
      "metadata": {
        "id": "ueom92WbBAhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "# Load the dataset from the saved location\n",
        "dataset_local = load_from_disk(\"multilingual_paradetox_test\")\n",
        "\n",
        "# Print some sample data\n",
        "print(dataset_local[\"en\"][0])\n"
      ],
      "metadata": {
        "id": "4UbbvBtEbEHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset from Hugging Face\n",
        "dataset = load_dataset(\"textdetox/multilingual_paradetox_test\")\n",
        "\n",
        "# Define the path in Google Drive\n",
        "save_path = \"/content/drive/MyDrive/...\n",
        "\"\n",
        "\n",
        "# Save dataset to Google Drive\n",
        "dataset.save_to_disk(save_path)\n",
        "\n",
        "print(f\"Dataset saved to {save_path}\")\n"
      ],
      "metadata": {
        "id": "1WyjU4qzb9HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"en\"].to_csv(\"/content/drive/MyDrive/...\")  # Save as CSV\n",
        "dataset[\"en\"].to_json(\"/content/drive/MyDrive/...\")  # Save as JSON\n"
      ],
      "metadata": {
        "id": "bIqkjnqrdLTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n"
      ],
      "metadata": {
        "id": "jNLgP557xxu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"s-nlp/paradetox\")"
      ],
      "metadata": {
        "id": "F5yFeELxxw0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds)"
      ],
      "metadata": {
        "id": "inzhQ9A-yJoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds.save_to_disk(\"s-nlp/paradetox\")\n"
      ],
      "metadata": {
        "id": "_oE61K2Ey9tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds[\"train\"].to_csv(\"/content/drive/MyDrive/...\")  # Save as CSV\n",
        "ds[\"train\"].to_json(\"/content/drive/MyDrive/...\")  # Save as JSON"
      ],
      "metadata": {
        "id": "rbSfAUsQ01B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"s-nlp/ru_paradetox\")"
      ],
      "metadata": {
        "id": "FEA3buSkBpho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds)\n",
        "ds.save_to_disk(\"s-nlp/paradetox\")"
      ],
      "metadata": {
        "id": "Al8HQtmeB2yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds[\"train\"].to_csv(\"/content/drive/MyDrive/...\")  # Save as CSV\n",
        "ds[\"train\"].to_json(\"/content/drive/MyDrive/...\")  # Save as JSON"
      ],
      "metadata": {
        "id": "oNObNjjTCLR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1\n"
      ],
      "metadata": {
        "id": "LsrEdd5EL9LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from googletrans import Translator\n",
        "\n",
        "# Load the dataset using `datasets`\n",
        "dataset = load_dataset(\"csv\", data_files=\"/content/drive/MyDrive/...\")[\"train\"]\n",
        "\n",
        "# Convert dataset to pandas DataFrame\n",
        "df_ru = dataset.to_pandas()\n",
        "\n",
        "# Initialize Google Translator\n",
        "translator = Translator()\n",
        "\n",
        "# Function to translate text\n",
        "def translate_text(text):\n",
        "    try:\n",
        "        return translator.translate(text, src='ru', dest='en').text\n",
        "    except Exception as e:\n",
        "        return text  # If translation fails, return original text\n",
        "\n",
        "# Apply translation to both columns\n",
        "df_ru[\"en_toxic_comment\"] = df_ru[\"ru_toxic_comment\"].apply(translate_text)\n",
        "df_ru[\"en_neutral_comment\"] = df_ru[\"ru_neutral_comment\"].apply(translate_text)\n",
        "\n",
        "# Drop Russian columns (optional)\n",
        "df_en = df_ru.drop(columns=[\"ru_toxic_comment\", \"ru_neutral_comment\"])\n",
        "\n",
        "# Save back to datasets format\n",
        "from datasets import Dataset\n",
        "dataset_en = Dataset.from_pandas(df_en)\n",
        "\n",
        "# Save to disk or Google Drive\n",
        "dataset_en.to_csv(\"/content/drive/MyDrive/...\", index=False)\n",
        "\n",
        "print(\"Translation complete. Dataset saved to Google Drive.\")\n"
      ],
      "metadata": {
        "id": "PJI6cDe1MXIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from googletrans import Translator\n",
        "import multiprocessing\n",
        "\n",
        "# Load your dataset\n",
        "file_path = \"/content/drive/MyDrive/...\"\n",
        "df_ru = pd.read_csv(file_path)\n",
        "\n",
        "# Initialize Google Translator\n",
        "translator = Translator()\n",
        "\n",
        "# Function to translate text\n",
        "def translate_text(text):\n",
        "    try:\n",
        "        return translator.translate(text, src='ru', dest='en').text\n",
        "    except Exception as e:\n",
        "        return text  # Return original text if translation fails\n",
        "\n",
        "# Function to translate a column in parallel\n",
        "def parallel_translate(column):\n",
        "    with multiprocessing.Pool(processes=8) as pool:  # Adjust number of processes as needed\n",
        "        return pool.map(translate_text, column)\n",
        "\n",
        "# Run parallel translation\n",
        "df_ru[\"en_toxic_comment\"] = parallel_translate(df_ru[\"ru_toxic_comment\"])\n",
        "df_ru[\"en_neutral_comment\"] = parallel_translate(df_ru[\"ru_neutral_comment\"])\n",
        "\n",
        "# Drop Russian columns (optional)\n",
        "df_en = df_ru.drop(columns=[\"ru_toxic_comment\", \"ru_neutral_comment\"])\n",
        "\n",
        "# Convert back to dataset format\n",
        "dataset_en = Dataset.from_pandas(df_en)\n",
        "\n",
        "# Save to Google Drive\n",
        "output_path = \"/content/drive/MyDrive/...\"\n",
        "dataset_en.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Translation complete. Dataset saved to {output_path}\")\n"
      ],
      "metadata": {
        "id": "JBuFJTbiETYG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}