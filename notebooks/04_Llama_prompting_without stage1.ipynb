{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhIHKEke6VQH"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes einops\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"...\")  # Replace with your real token"
      ],
      "metadata": {
        "id": "3n_ZH8sT-pg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# Load model in 8-bit to fit into Colab GPU\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_8bit=True\n",
        ")\n",
        "\n",
        "# Set to eval mode\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "EhLQRwwm-V7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def generate_response(prompt, max_new_tokens=100):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Strip everything before \"Detoxified:\" and clean up\n",
        "    if \"Detoxified:\" in decoded:\n",
        "        detoxified_part = decoded.split(\"Detoxified:\")[1]\n",
        "        # Remove anything that looks like leftover prompt text\n",
        "        detoxified_part = detoxified_part.replace(\"[/INST]\", \"\").strip()\n",
        "        return detoxified_part.split(\"\\n\")[0].strip()\n",
        "\n",
        "    return decoded.strip()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Example toxic input\n",
        "toxic_sentence = \"I see nothing wrong with calling an assholeasshole.\"\n",
        "\n",
        "# Experiment 3-style prompt\n",
        "prompt = f\"<s>[INST] Rewrite the following sentence to be respectful and non-offensive, without changing its meaning.\\n\\nToxic: \\\"{toxic_sentence}\\\"\\nDetoxified: [/INST]\"\n",
        "\n",
        "\n",
        "# Generate and print\n",
        "response = generate_response(prompt)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "LKdJhJSnmZ17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "L6ShQ51moMu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")\n",
        "\n",
        "# Batch generation with tqdm\n",
        "tqdm.pandas()\n",
        "def prompt_and_generate(toxic_sentence):\n",
        "    prompt = f\"<s>[INST] Rewrite the following sentence to be respectful and non-offensive, without changing its meaning.\\n\\nToxic: \\\"{toxic_sentence}\\\"\\nDetoxified: [/INST]\"\n",
        "    return generate_response(prompt)\n",
        "\n",
        "# Overwrite the 'generated_output' column with detoxified text\n",
        "df[\"generated_output\"] = df[\"toxic_sentence\"].progress_apply(prompt_and_generate)\n",
        "\n",
        "# Save final result\n",
        "output_path = \"/content/drive/MyDrive...\"\n",
        "df.to_csv(output_path, index=False)\n",
        "print(f\"âœ… Saved: {output_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "oHdhaj3Uoh5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/drive/MyDrive/...')"
      ],
      "metadata": {
        "id": "-d2SsDPPjz9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluation:"
      ],
      "metadata": {
        "id": "nqARfLyB2beU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")"
      ],
      "metadata": {
        "id": "eraDf6-W2cGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score evaluate sacrebleu\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "P, R, F1 = bert_score(\n",
        "    cands=df[\"generated_output\"].tolist(),\n",
        "    refs=df[\"neutral_reference\"].tolist(),\n",
        "    lang=\"en\"\n",
        ")\n",
        "\n",
        "print(f\"BERTScore F1: {F1.mean():.4f}\")"
      ],
      "metadata": {
        "id": "VlzmHipf5Dol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers sacrebleu"
      ],
      "metadata": {
        "id": "IbyBm3W158hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")  # adjust path if needed\n",
        "\n",
        "# Rename columns to standard names for code consistency\n",
        "df.rename(columns={\n",
        "    \"toxic_sentence\": \"input\",\n",
        "    \"generated_output\": \"prediction\",\n",
        "    \"neutral_reference\": \"reference\"\n",
        "}, inplace=True)"
      ],
      "metadata": {
        "id": "qXSHIkJ56DHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "class ToxicityClassifierPipeline:\n",
        "    def __init__(self, binary_model_path, fine_model_path, device=\"cuda\"):\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.tokenizer_binary = AutoTokenizer.from_pretrained(binary_model_path)\n",
        "        self.model_binary = AutoModelForSequenceClassification.from_pretrained(binary_model_path).to(self.device)\n",
        "\n",
        "        self.tokenizer_fine = AutoTokenizer.from_pretrained(fine_model_path)\n",
        "        self.model_fine = AutoModelForSequenceClassification.from_pretrained(fine_model_path).to(self.device)\n",
        "\n",
        "        self.label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "        self.label_to_explanation = {\n",
        "            \"toxic\": \"This sentence contains general toxic language.\",\n",
        "            \"severe_toxic\": \"This sentence contains extreme hostility or verbal abuse.\",\n",
        "            \"obscene\": \"This sentence contains obscene or vulgar language.\",\n",
        "            \"threat\": \"This sentence contains a threat or implied violence.\",\n",
        "            \"insult\": \"This sentence includes personal insults or demeaning language.\",\n",
        "            \"identity_hate\": \"This sentence attacks someone based on identity (e.g. race, gender, religion).\"\n",
        "        }\n",
        "\n",
        "    def __call__(self, comment, threshold=0.5):\n",
        "        inputs = self.tokenizer_binary(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_binary(**inputs)\n",
        "            probs = torch.softmax(outputs.logits, dim=1)\n",
        "            toxic_prob = probs[:, 1].item()\n",
        "\n",
        "        binary_pred = \"toxic\" if toxic_prob >= threshold else \"non-toxic\"\n",
        "\n",
        "        if binary_pred == \"non-toxic\":\n",
        "            return {\"binary\": binary_pred, \"subtypes\": None, \"toxic_prob\": toxic_prob, \"explanation\": None}\n",
        "\n",
        "        inputs = self.tokenizer_fine(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_fine(**inputs)\n",
        "            fine_probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]\n",
        "\n",
        "        subtypes = {\n",
        "            label: round(float(prob), 2)\n",
        "            for label, prob in zip(self.label_cols, fine_probs)\n",
        "            if prob >= threshold\n",
        "        }\n",
        "\n",
        "        explanation_parts = [self.label_to_explanation[label] for label in subtypes]\n",
        "        explanation = \" \".join(explanation_parts) if explanation_parts else None\n",
        "\n",
        "        return {\n",
        "            \"binary\": binary_pred,\n",
        "            \"subtypes\": subtypes,\n",
        "            \"toxic_prob\": toxic_prob,\n",
        "            \"explanation\": explanation\n",
        "        }"
      ],
      "metadata": {
        "id": "uVTcSkRS6Q2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = ToxicityClassifierPipeline(\n",
        "    binary_model_path=\"/content/drive/MyDrive/modernbert-binary-toxic\",\n",
        "    fine_model_path=\"/content/drive/MyDrive/modernbert-fine-toxic\"\n",
        ")"
      ],
      "metadata": {
        "id": "D306gfMa6T1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_toxic(text):\n",
        "    result = pipeline(text)\n",
        "    return result[\"binary\"] == \"toxic\"\n",
        "\n",
        "df[\"STA\"] = [0 if is_toxic(pred) else 1 for pred in df[\"prediction\"]]"
      ],
      "metadata": {
        "id": "2KsaAI9E6Xgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load multilingual sentence similarity model\n",
        "labse = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
        "\n",
        "emb_input = labse.encode(df[\"input\"].tolist(), convert_to_tensor=True)\n",
        "emb_pred = labse.encode(df[\"prediction\"].tolist(), convert_to_tensor=True)\n",
        "\n",
        "similarities = util.cos_sim(emb_input, emb_pred).diagonal().tolist()\n",
        "df[\"SIM\"] = similarities"
      ],
      "metadata": {
        "id": "Yvxa5yUm6cU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu.metrics import CHRF\n",
        "\n",
        "chrf = CHRF()\n",
        "chrf_scores = [\n",
        "    chrf.sentence_score(pred, [ref]).score / 100\n",
        "    for pred, ref in zip(df[\"prediction\"], df[\"reference\"])\n",
        "]\n",
        "df[\"CHRF\"] = chrf_scores"
      ],
      "metadata": {
        "id": "o_SwA2Lq68MN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Final J-score is the average of STA, SIM, and CHRF\n",
        "df[\"J-score\"] = (df[\"STA\"] + df[\"SIM\"] + df[\"CHRF\"]) / 3\n",
        "\n",
        "# Print final average\n",
        "print(f\"âœ… J-score (mean over all examples): {df['J-score'].mean():.4f}\")"
      ],
      "metadata": {
        "id": "5eZKwy8f7Dh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"/content/llama_evaluation_results.csv\", index=False)"
      ],
      "metadata": {
        "id": "VWzP28EV7H0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/llama_evaluation_results.csv')"
      ],
      "metadata": {
        "id": "4FMPOzmS7M4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Metric means from previously computed columns\n",
        "df[\"J-score\"] = (df[\"STA\"] + df[\"SIM\"] + df[\"CHRF\"]) / 3\n",
        "\n",
        "bert_f1 = F1.mean().item()                     # BERTScore F1\n",
        "chrf_score = df[\"CHRF\"].mean()                # CHRF\n",
        "sta_score = df[\"STA\"].mean()                  # STA\n",
        "sim_score = df[\"SIM\"].mean()                  # SIM\n",
        "j_score = df[\"J-score\"].mean()                # J-score\n",
        "\n",
        "# Calculate overall average metrics\n",
        "average_scores = {\n",
        "    \"Metric\": [\"BERTScore F1\", \"STA\", \"SIM\", \"CHRF\", \"Average J-score\"],\n",
        "    \"Score\": [\n",
        "        bert_f1,\n",
        "        sta_score,\n",
        "        sim_score,\n",
        "        chrf_score,\n",
        "        j_score\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame and export\n",
        "summary_df = pd.DataFrame(average_scores)\n",
        "summary_df.to_csv(\"/content/drive/MyDrive/...\", index=False)"
      ],
      "metadata": {
        "id": "V5qFvoJ27Sl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/drive/MyDrive/...')"
      ],
      "metadata": {
        "id": "C0SNWPJQ7X-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}