{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N7QgL18DVvy"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets peft accelerate bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3elZQbHht6k"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUTrxtw2ERR0"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeApkhxMD0SE"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"s-nlp/paradetox\")\n",
        "train_data = dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsvhHH3A17B1"
      },
      "outputs": [],
      "source": [
        "# 2. Format examples\n",
        "def format_example(example):\n",
        "    return {\n",
        "        \"input_text\": f\"detoxify: {example['en_toxic_comment']}\",\n",
        "        \"labels\": example[\"en_neutral_comment\"]\n",
        "    }\n",
        "\n",
        "train_data = train_data.map(format_example, remove_columns=train_data.column_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTiXka582EP9"
      },
      "outputs": [],
      "source": [
        "# 3. Tokenization with padding mask\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/mt0-base\")\n",
        "\n",
        "# ✅ Add a real padding token instead of using eos\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "def tokenize(example):\n",
        "    model_input = tokenizer(\n",
        "        example[\"input_text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        label = tokenizer(\n",
        "            example[\"labels\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=256\n",
        "        )\n",
        "\n",
        "    # ✅ Now [PAD] token will be correctly masked\n",
        "    model_input[\"labels\"] = [\n",
        "        token if token != tokenizer.pad_token_id else -100\n",
        "        for token in label[\"input_ids\"]\n",
        "    ]\n",
        "\n",
        "    return model_input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JwLksEl2H-n"
      },
      "outputs": [],
      "source": [
        "# 4. Apply tokenization\n",
        "tokenized_data = train_data.map(tokenize, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBhjfRCo8Mud"
      },
      "outputs": [],
      "source": [
        "print(tokenized_data[0][\"labels\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCI_bmvKFSvY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.set_default_dtype(torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0rfAp72FYRP"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainerCallback\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ZLossCallback(TrainerCallback):\n",
        "    def on_step_end(self, args, state, control, logs=None, **kwargs):\n",
        "        logs = logs or {}\n",
        "        if \"loss\" in logs:\n",
        "            logits = kwargs[\"outputs\"].logits  # get logits from model forward\n",
        "            log_z = logits.view(-1, logits.size(-1)).logsumexp(-1)\n",
        "            z_loss = 1e-4 * torch.mean(log_z ** 2)\n",
        "            logs[\"loss\"] += z_loss.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRwVXLfWEggW"
      },
      "outputs": [],
      "source": [
        "# 5. Load model\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/mt0-base\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fdImOQzGGkD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import types\n",
        "from transformers import MT5ForConditionalGeneration\n",
        "\n",
        "# Store the original class's forward method\n",
        "original_forward_fn = MT5ForConditionalGeneration.forward\n",
        "\n",
        "def patched_forward(self, *args, **kwargs):\n",
        "    # Call the original model class method\n",
        "    outputs = original_forward_fn(self, *args, **kwargs)\n",
        "\n",
        "    # Get logits and base loss\n",
        "    logits = outputs.logits\n",
        "    base_loss = outputs.loss\n",
        "\n",
        "    # Add z-loss penalty\n",
        "    log_z = logits.view(-1, logits.size(-1)).logsumexp(-1)\n",
        "    z_loss = 1e-4 * torch.mean(log_z ** 2)\n",
        "    total_loss = base_loss + z_loss\n",
        "\n",
        "    # Return updated output\n",
        "    return type(outputs)(loss=total_loss, logits=logits)\n",
        "\n",
        "# Inject the safe patch\n",
        "model.forward = types.MethodType(patched_forward, model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqHPPZQM2XEk"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mt0_paradetox_en_fixed\",\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=4,\n",
        "    learning_rate=1e-5,\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,\n",
        "    fp16=False,\n",
        "    report_to=\"none\"  # disables wandb\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[ZLossCallback()]\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmMFIFBb2gIc"
      },
      "outputs": [],
      "source": [
        "# 7. Train\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1adUI4xM3dbr"
      },
      "outputs": [],
      "source": [
        "# Check first 5 label sequences\n",
        "for i in range(5):\n",
        "    print(tokenized_data[i][\"labels\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q9Y0y_JINq-"
      },
      "source": [
        "Code for L4 GPU (No AMP, No Patching)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUXQy-GVIQuu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.set_default_dtype(torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qqlyj9J7ITWY"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load ParadeTox English split\n",
        "train_data = load_dataset(\"s-nlp/paradetox\", split=\"train\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRCGfd1EIhxI"
      },
      "outputs": [],
      "source": [
        "# 2. Format examples\n",
        "def format_example(example):\n",
        "    return {\n",
        "        \"input_text\": f\"detoxify: {example['en_toxic_comment']}\",\n",
        "        \"labels\": example[\"en_neutral_comment\"]\n",
        "    }\n",
        "\n",
        "train_data = train_data.map(format_example, remove_columns=train_data.column_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB-pEChmIjno"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/mt0-base\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Required for MT0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9SSiK1fIlTx"
      },
      "outputs": [],
      "source": [
        "def tokenize(example):\n",
        "    model_input = tokenizer(\n",
        "        example[\"input_text\"],\n",
        "        text_target=example[\"labels\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "    )\n",
        "    return model_input\n",
        "\n",
        "tokenized_data = train_data.map(tokenize, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyi9A3ywIqF3"
      },
      "outputs": [],
      "source": [
        "from transformers import MT5ForConditionalGeneration\n",
        "\n",
        "model = MT5ForConditionalGeneration.from_pretrained(\"bigscience/mt0-base\")\n",
        "model.resize_token_embeddings(len(tokenizer))  # Required if you set pad_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khlj4w7jIw_7"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/...\",  #  save here\n",
        "    save_total_limit=1,         # keep only the last 2 checkpoints\n",
        "    save_steps=1000,             # save every 500 steps\n",
        "    logging_dir=\"/content/drive/MyDrive/...\",  # optional: for TensorBoard logs\n",
        "    logging_steps=100,\n",
        "    #evaluation_strategy=\"no\",   # change to \"steps\" if you want eval during training\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    fp16=False,                 # or True if you use mixed precision\n",
        "    report_to=\"none\",           # disable wandb\n",
        "    save_strategy=\"steps\",      # required to enable checkpoint saving\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dbcg_dpNI5mZ"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data,\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIh49ExiI8Iu"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reCENVxnv4tJ"
      },
      "outputs": [],
      "source": [
        "# Mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mvNQb5UdmhBE"
      },
      "outputs": [],
      "source": [
        "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/...\"\n",
        "\n",
        "# Make sure model is already trained and exists in memory here\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw559B9kmo4I"
      },
      "outputs": [],
      "source": [
        "!ls /content/drive/MyDrive/mt0_paradetox_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njJ9H1GJmwiM"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/drive/MyDrive/...\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = MT5ForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "print(\"Model and tokenizer loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6nEM6BCwQn3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ7xKImayQt-"
      },
      "source": [
        "calling the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFGH1E-Mx-XE"
      },
      "outputs": [],
      "source": [
        "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/...\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = MT5ForConditionalGeneration.from_pretrained(model_path).to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moY7MDneyBM6"
      },
      "outputs": [],
      "source": [
        "# Add the prompt prefix\n",
        "inputs = [\"detoxify: \" + str(x) for x in df[\"toxic_sentence\"].tolist()]\n",
        "\n",
        "# Tokenize\n",
        "tokenized = tokenizer(\n",
        "    inputs,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=256\n",
        ").to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRSiWJZ4y6v6"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids=tokenized[\"input_ids\"],\n",
        "        attention_mask=tokenized[\"attention_mask\"],\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZKJfBZay9n3"
      },
      "outputs": [],
      "source": [
        "df[\"generated_output\"] = decoded_outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlalYHfey-77"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"/content/drive/MyDrive/...\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTTzpsKrzJ33"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/drive/MyDrive/...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NffZHSeg0WOY"
      },
      "source": [
        " evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeGGtJfM0ZMS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZ2P0ro20bKk"
      },
      "outputs": [],
      "source": [
        "!pip install bert-score evaluate sacrebleu\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "P, R, F1 = bert_score(\n",
        "    cands=df[\"generated_output\"].tolist(),\n",
        "    refs=df[\"neutral_reference\"].tolist(),\n",
        "    lang=\"en\"\n",
        ")\n",
        "\n",
        "print(f\"BERTScore F1: {F1.mean():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDdyjEnpLBDC"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers sacrebleu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coMQ3LSx1MAF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")  # adjust path if needed\n",
        "\n",
        "# Rename columns to standard names for code consistency\n",
        "df.rename(columns={\n",
        "    \"toxic_sentence\": \"input\",\n",
        "    \"generated_output\": \"prediction\",\n",
        "    \"neutral_reference\": \"reference\"\n",
        "}, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kqiGKvsdMgq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "class ToxicityClassifierPipeline:\n",
        "    def __init__(self, binary_model_path, fine_model_path, device=\"cuda\"):\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.tokenizer_binary = AutoTokenizer.from_pretrained(binary_model_path)\n",
        "        self.model_binary = AutoModelForSequenceClassification.from_pretrained(binary_model_path).to(self.device)\n",
        "\n",
        "        self.tokenizer_fine = AutoTokenizer.from_pretrained(fine_model_path)\n",
        "        self.model_fine = AutoModelForSequenceClassification.from_pretrained(fine_model_path).to(self.device)\n",
        "\n",
        "        self.label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "        self.label_to_explanation = {\n",
        "            \"toxic\": \"This sentence contains general toxic language.\",\n",
        "            \"severe_toxic\": \"This sentence contains extreme hostility or verbal abuse.\",\n",
        "            \"obscene\": \"This sentence contains obscene or vulgar language.\",\n",
        "            \"threat\": \"This sentence contains a threat or implied violence.\",\n",
        "            \"insult\": \"This sentence includes personal insults or demeaning language.\",\n",
        "            \"identity_hate\": \"This sentence attacks someone based on identity (e.g. race, gender, religion).\"\n",
        "        }\n",
        "\n",
        "    def __call__(self, comment, threshold=0.5):\n",
        "        inputs = self.tokenizer_binary(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_binary(**inputs)\n",
        "            probs = torch.softmax(outputs.logits, dim=1)\n",
        "            toxic_prob = probs[:, 1].item()\n",
        "\n",
        "        binary_pred = \"toxic\" if toxic_prob >= threshold else \"non-toxic\"\n",
        "\n",
        "        if binary_pred == \"non-toxic\":\n",
        "            return {\"binary\": binary_pred, \"subtypes\": None, \"toxic_prob\": toxic_prob, \"explanation\": None}\n",
        "\n",
        "        inputs = self.tokenizer_fine(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_fine(**inputs)\n",
        "            fine_probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]\n",
        "\n",
        "        subtypes = {\n",
        "            label: round(float(prob), 2)\n",
        "            for label, prob in zip(self.label_cols, fine_probs)\n",
        "            if prob >= threshold\n",
        "        }\n",
        "\n",
        "        explanation_parts = [self.label_to_explanation[label] for label in subtypes]\n",
        "        explanation = \" \".join(explanation_parts) if explanation_parts else None\n",
        "\n",
        "        return {\n",
        "            \"binary\": binary_pred,\n",
        "            \"subtypes\": subtypes,\n",
        "            \"toxic_prob\": toxic_prob,\n",
        "            \"explanation\": explanation\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isyHkR0K1e2c"
      },
      "outputs": [],
      "source": [
        "pipeline = ToxicityClassifierPipeline(\n",
        "    binary_model_path=\"/content/drive/MyDrive/...\",\n",
        "    fine_model_path=\"/content/drive/MyDrive/...\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-186Ub31i-l"
      },
      "outputs": [],
      "source": [
        "def is_toxic(text):\n",
        "    result = pipeline(text)\n",
        "    return result[\"binary\"] == \"toxic\"\n",
        "\n",
        "df[\"STA\"] = [0 if is_toxic(pred) else 1 for pred in df[\"prediction\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QL_qKUf1mXX"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load multilingual sentence similarity model\n",
        "labse = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
        "\n",
        "emb_input = labse.encode(df[\"input\"].tolist(), convert_to_tensor=True)\n",
        "emb_pred = labse.encode(df[\"prediction\"].tolist(), convert_to_tensor=True)\n",
        "\n",
        "similarities = util.cos_sim(emb_input, emb_pred).diagonal().tolist()\n",
        "df[\"SIM\"] = similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLL_E-Ux1qKt"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import CHRF\n",
        "\n",
        "chrf = CHRF()\n",
        "chrf_scores = [\n",
        "    chrf.sentence_score(pred, [ref]).score / 100\n",
        "    for pred, ref in zip(df[\"prediction\"], df[\"reference\"])\n",
        "]\n",
        "df[\"CHRF\"] = chrf_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHD08FfM1uV1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Final J-score is the average of STA, SIM, and CHRF\n",
        "df[\"J-score\"] = (df[\"STA\"] + df[\"SIM\"] + df[\"CHRF\"]) / 3\n",
        "\n",
        "# Print final average\n",
        "print(f\"✅ J-score (mean over all examples): {df['J-score'].mean():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Cm2SfWz1-pR"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"/content/...\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihuu_xZs2Hom"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/...')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCNtFuPo3tmD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Metric means from previously computed columns\n",
        "df[\"J-score\"] = (df[\"STA\"] + df[\"SIM\"] + df[\"CHRF\"]) / 3\n",
        "\n",
        "bert_f1 = F1.mean().item()                     # BERTScore F1\n",
        "chrf_score = df[\"CHRF\"].mean()                # CHRF\n",
        "sta_score = df[\"STA\"].mean()                  # STA\n",
        "sim_score = df[\"SIM\"].mean()                  # SIM\n",
        "j_score = df[\"J-score\"].mean()                # J-score\n",
        "\n",
        "# Calculate overall average metrics\n",
        "average_scores = {\n",
        "    \"Metric\": [\"BERTScore F1\", \"STA\", \"SIM\", \"CHRF\", \"Average J-score\"],\n",
        "    \"Score\": [\n",
        "        bert_f1,\n",
        "        sta_score,\n",
        "        sim_score,\n",
        "        chrf_score,\n",
        "        j_score\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame and export\n",
        "summary_df = pd.DataFrame(average_scores)\n",
        "summary_df.to_csv(\"/content/drive/...\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRUXdF6y5ktM"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/drive/...')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aURu0H24jw6G"
      },
      "source": [
        "✅mt0-base finetuned with stage one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in80g-7Nj1I_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0X2YiXxw9Ye"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "\n",
        "class ToxicityClassifierPipeline:\n",
        "    def __init__(self, binary_model_path, fine_model_path, device=\"cuda\"):\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.tokenizer_binary = AutoTokenizer.from_pretrained(binary_model_path)\n",
        "        self.model_binary = AutoModelForSequenceClassification.from_pretrained(binary_model_path).to(self.device)\n",
        "\n",
        "        self.tokenizer_fine = AutoTokenizer.from_pretrained(fine_model_path)\n",
        "        self.model_fine = AutoModelForSequenceClassification.from_pretrained(fine_model_path).to(self.device)\n",
        "\n",
        "        self.label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "        self.label_to_explanation = {\n",
        "            \"toxic\": \"This sentence contains general toxic language.\",\n",
        "            \"severe_toxic\": \"This sentence includes severe verbal abuse.\",\n",
        "            \"obscene\": \"This sentence contains obscene or vulgar language.\",\n",
        "            \"threat\": \"This sentence includes a threat or incites violence.\",\n",
        "            \"insult\": \"This sentence includes personal insults.\",\n",
        "            \"identity_hate\": \"This sentence attacks a person's identity.\"\n",
        "        }\n",
        "\n",
        "    def __call__(self, comment, threshold=0.5):\n",
        "        inputs = self.tokenizer_binary(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_binary(**inputs)\n",
        "            probs = torch.softmax(outputs.logits, dim=1)\n",
        "            toxic_prob = probs[:, 1].item()\n",
        "\n",
        "        binary_pred = \"toxic\" if toxic_prob >= threshold else \"non-toxic\"\n",
        "\n",
        "        if binary_pred == \"non-toxic\":\n",
        "            return {\"binary\": binary_pred, \"explanation\": None}\n",
        "\n",
        "        inputs = self.tokenizer_fine(comment, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model_fine(**inputs)\n",
        "            fine_probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]\n",
        "\n",
        "        explanation_parts = [\n",
        "            self.label_to_explanation[label]\n",
        "            for label, prob in zip(self.label_cols, fine_probs)\n",
        "            if prob >= threshold\n",
        "        ]\n",
        "\n",
        "        explanation = \" \".join(explanation_parts) if explanation_parts else \"This sentence contains toxic language.\"\n",
        "        return {\n",
        "            \"binary\": binary_pred,\n",
        "            \"explanation\": explanation\n",
        "        }\n",
        "\n",
        "# Instantiate the classifier\n",
        "pipeline = ToxicityClassifierPipeline(\n",
        "    binary_model_path=\"/content/drive/MyDrive/...\",\n",
        "    fine_model_path=\"/content/drive/MyDrive/...\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El1U2rNCuq2D"
      },
      "outputs": [],
      "source": [
        "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/...\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = MT5ForConditionalGeneration.from_pretrained(model_path).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kk2xp9pHxS3T"
      },
      "outputs": [],
      "source": [
        "def build_mt0_prompt(toxic_sentence, explanation):\n",
        "    return (\n",
        "        f\"Detoxify the following sentence while preserving its meaning. \"\n",
        "        f\"The explanation below describes why the sentence is considered toxic:\\n\"\n",
        "        f\"Toxic: {toxic_sentence}\\n\"\n",
        "        f\"Explanation: {explanation}\\n\"\n",
        "        f\"Detoxified:\"\n",
        "    )\n",
        "\n",
        "def generate_mt0_detox(prompt, model, tokenizer):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=60,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2\n",
        "        )\n",
        "\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # ✅ Remove prompt from output if echoed\n",
        "    if \"Detoxified:\" in decoded:\n",
        "        detoxified = decoded.split(\"Detoxified:\")[-1].strip()\n",
        "    else:\n",
        "        detoxified = decoded.strip()\n",
        "\n",
        "    return detoxified\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIWoZHpQxboc"
      },
      "outputs": [],
      "source": [
        "# Load test file\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/...\")\n",
        "\n",
        "# Generate new detox outputs\n",
        "outputs_mt0_stage1 = []\n",
        "\n",
        "for text in df[\"toxic_sentence\"]:\n",
        "    stage1 = pipeline(text)\n",
        "    explanation = stage1[\"explanation\"] if stage1[\"explanation\"] else \"This sentence contains toxic language.\"\n",
        "    prompt = build_mt0_prompt(text, explanation)\n",
        "    detox = generate_mt0_detox(prompt, mt0_model, mt0_tokenizer)\n",
        "    outputs_mt0_stage1.append(detox)\n",
        "\n",
        "# Save new outputs\n",
        "df[\"mt0_base_output_stage1\"] = outputs_mt0_stage1\n",
        "df.to_csv(\"/content/drive/MyDrive/...\", index=False)\n",
        "\n",
        "print(\"Saved to: mt0_base_retrain_output_with_stage1.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}